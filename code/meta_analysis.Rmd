---
output: html_document
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 80
---

```{=html}
<style type="text/css">
.main-container {
  max-width: 800px;
  margin-left: auto;
  margin-right: auto;
}
</style>
```

### NOTES:<br>
**I may be able to impute standard deviations for the studies that don't have them, as was done in Bishop & Nakagawa (2021). I found that bit tricky though when testing it out**<br>
**It would be good to use the renv package, but I'll wait until this analysis/code is sorted before adding in that element**<br>
**I'm happy to use Quarto if you think that would be better than Markdown**<br>


# Dung beetles increase plant growth: a meta-analysis

Our two aims with this project are to conduct a systematic review and then meta-analysis to assess:

## 1) When dung beetles access dung, does plant growth increase in the immediate area?

## 2) Are there ecological and experimental factors (moderators) that influence this plant growth?
<br>

```{r Load libraries and set parent directory}
#| include: false
#| message: false
#| warning: false

# Load libraries
library("broom")
library("dmetar")
library("dplyr")
library("here")
library("knitr")
library("matrixcalc")
library("metaAidR")
library("metafor")
library("multcompView")
library("orchaRd")   # github.com/itchyshin/orchard_plot
library("readxl")
library("tidyverse")
library("wesanderson")

# Set the parent directory so that the relative paths used later work properly
here()   

```

Firstly we conducted a systematic review following the PRISMA 2020 method (Page et al., 2021).<br>
<br>

```{r}
#| echo: false
#| out.width: '100%'

include_graphics(here("figures/prisma_flow_diagram.png"))   # Insert the PRISMA Flow Diagram using knitr

```

We then extracted data from those reports to conduct a meta-analysis.<br>

```{r Import data}
#| include: false

data <- read_excel(here("data/meta_analysis_data.xlsx"),
                   na = c("", "NA"))   # Import dataset from the data folder

```


```{r Prep data for analysis}
#| include: false

# Turn the required columns into factors:
data <-
data %>%
  mutate(across(.cols = c(STUDYID,
                          ACCESSION_ID,
                          MULTIPLE_ENDPOINT_CLUSTERID,
                          SHAREDCONTROL_CLUSTERID,
                          TREATMENT,
                          FUNCTIONAL_GROUP,
                          SPECIES_RICHNESS,
                          BEETLES_CONFINED_TO_CAGE,
                          DUNG_TYPE,
                          DUNG_FROZEN_PRIOR_TO_ALIQUOTTING,
                          DUNG_REMOVED_AFTER_BEETLE_ACTIVITY,
                          PLANTS_GERMINATED_BEFORE_DUNG_ADDITION,
                          PLANT_TYPE,
                          CLADE,
                          RESPONSE_AREA,
                          RESPONSE_MEASUREMENT,
                          RESPONSE,
                          CONTROL_TREATMENT,
                          EXPERIMENT_SETTING), 
                as.factor))

sapply(data, class)   # Check that the required columns are now factors

# Calculate a variable for the grams of dung per cm squared of each plot/mesocosm
data$GRAMS_DUNG_PER_CM_SQUARED <- data$DUNG_QUANTITY_IN_GRAMS / data$SURFACE_AREA_CM_SQUARED
# Calculate a variable for the grams of dung per beetle
data$GRAMS_DUNG_PER_BEETLE <- data$DUNG_QUANTITY_IN_GRAMS / data$NUMBER_OF_BEETLES

```

Our dataset features correlated, or dependent, outcomes within studies (Noble et al., 2017). This correlation can be due to:<br> 
**Shared control** - Where multiple treatments are compared to the same control (e.g. Manning et al., 2017).<br>
**Shared measurements** - Where more than one outcome/measurement is reported for the same study (e.g. Johnson et al., 2016).<br>
**Within-study temporal correlation** - Where the effect is measured at multiple timepoints (e.g. Nervo et al., 2017).<br>
For more information on non-dependent effect sizes see Nakagawa & Santos (2012), Noble et al. (2017) and Bishop & Nakagawa (2021).<br>
<br>
We deal with the **shared control** aspect of our dataset before calculating the effect sizes.<br>
<br>
We now modify the control sample size (CONTROL_N) to account for those studies with shared controls (Bishop and Nagakawa, 2021).
To do this we divide the sample size of each control (CONTROL_N) by the number of shared controls within its study (SHAREDCONTROL_COUNT). 
This creates a SHARED_CONTROL_N value which is then used (in escalc) to calculate the effect size for each outcome.

```{r Dealing with shared controls}
#| include: false

data <- data %>%
  group_by(STUDYID) %>%
  mutate(SHAREDCONTROL_COUNT = n_distinct(SHAREDCONTROL_CLUSTERID),   # Count the number of unique control groups within each study
         SHARED_CONTROL_N = CONTROL_N / SHAREDCONTROL_COUNT)   # Divide the sample size of each control (CONTROL_N) by the number of shared controls within its study

```

We then use measures of plant growth between 'dung+beetles' and 'dung only' treatments to calculate effect sizes. Effect sizes were calculated using a log transformed ratio of means, or log response ratio (lnRR) (Hedges et al., 1999). This is a usefull measurement as we can determine a percentage increase in plant growth. The metafor package was used to calculate the effect sizes (Viechtbauer 2010). For measure = "ROM", the log is taken of the ratio of means, which makes this outcome measure symmetric around 0 and yields a corresponding sampling distribution that is closer to normality (Viechtbauer 2010).<br> 

```{r Calculate effect size}
#| include: false

# Calculate an Effect Size (yi) and corresponding Sampling Variance (vi)
data1 <- escalc(measure="ROM",   # The effect size calculated will be log transformed ratio of means (metafor::escalc)
                m1i=TREATMENT_MEAN,   # m1i = the treatment mean (e.g. dung+beetles or dung_only)
                sd1i=TREATMENT_SD,   # sd1i = the treatment st.dev
                n1i=TREATMENT_N,   # n1i = the treatment sample size
                m2i=CONTROL_MEAN,   # m2i = the control mean (e.g. dung_only or soil_only)    
                sd2i=CONTROL_SD,   # sd2i = the control st.dev  
                n2i=SHARED_CONTROL_N,   # n2i = the control sample size (here we used our modified control sample size)
                data=data,
                slab=paste(STUDYID, YEAR, sep=", "))

# Rename the Effect Size (yi) and corresponding Sampling Variance (vi)columns to make them more intuitive to read 
colnames(data1)[which(names(data1) == "yi")] <- "effect_size"   # The Effect Size column is now labelled 'effect_size'
colnames(data1)[which(names(data1) == "vi")] <- "effect_size_variance"   # The Sampling Variance is now labelled 'effect_size_variance'

# Calculate a standard error (sqrt(effect_size_variance)) and a precision value (1/standard error) (Bishop & Nakagawa, 2021)
data1$se <- sqrt(data1$effect_size_variance)
data1$precision <- 1/data1$se

# Isolate just our treatments of interest 
# We just want to compare dung+beetles treatments with dung_only controls
data1 <- filter(data1, 
                TREATMENT == "dung+beetles",   # Isolate the "dung+beetles" treatments from the TREATMENT column
                CONTROL_TREATMENT == "dung_only")   # AND isolate the "dung_only+beetles" treatments from the CONTROL_TREATMENT column   

data2 <- data1   # Use data2 later for the un-weighted meta-analysis

# Filter out the studies that don't have effect sizes
data1 <- 
data1 %>% 
  drop_na(effect_size, effect_size_variance)   # Remove all studies that have 'NA' in the effect_size or effect_size_variance columns

```


As the dataset also contains non-independence in the form of **Shared measurements** and **Within-study temporal correlation** we create a value called MULTIPLE_ENDPOINT_CLUSTERID.
Whenever multiple plants traits were measured from the same plant at the same time, or when a single plant trait was measured from the same plant at different times, those outcomes received the same MULTIPLE_ENDPOINT_CLUSTERID. Sometimes studies had both Shared measurements and Within-study temporal correlation (e.g. Miranda et al., 1998).<br>

To deal with non-independence we use multi-level meta-analytic models with variance–covariance matrices.<br>
We generate variance-covariance matrices based on a clustered variable, which was MULTIPLE_ENDPOINT_CLUSTERID.<br>

As a sensitivity test, we produce three matrices, with effect sizes from the same experiment assumed to have a correlation of either 0.25, 0.5 or 0.75. A correlation value of 0 indicates no correlation and 1 a perfect correlation (Dougherty et al., 2022).<br>


```{r Correlation 0.25}
#| include: false

# Arrange data1 by STUDYID (ascending) so that the values from the matrices are associated with the correct effect sizes
data1 <- data1[order(data1$STUDYID), ]

# Generate the variance-covariance matrices using the metaAidR::make_VCV_matrix function
### Assumed correlation value = 0.25 correlation (low correlation)
variance.covariance.matrices_0.25 <- 
  make_VCV_matrix(data = data1,
                  V = "effect_size_variance",   # The variable containing effect size variances
                  cluster = "MULTIPLE_ENDPOINT_CLUSTERID",   # The variable indicating which effects belong to the same cluster. Outcomes with the same 'MULTIPLE_ENDPOINT_CLUSTERID' are assumed to be nonindependent (correlated).
                  obs = "ACCESSION_ID",   # The variable containing individual IDs for each value in V = "effect_size_variance"
                  type = "vcv",   # "vcv" indicates that a full variance-covariance matrix is needed for the non-independent blocks of variance values.
                  rho = 0.25)   # Setting the assumed correlation value at 0.25
is.positive.definite(variance.covariance.matrices_0.25)   # For a positive definite matrix, the eigenvalues should be positive, therefore we expect this function to return the output of 'TRUE'


model1_corr0.25 <- rma.mv(yi = effect_size,
                          V = variance.covariance.matrices_0.25,
                          random = list(~1 | STUDYID,   # Level 3: The highest grouping variable
                                        ~1 | ACCESSION_ID),   # Level 2: The second-highest grouping variable - nested within level 3
                   data = data1,
                   method = "REML")   # The restricted maximum-likelihood method is used to estimate the model parameters
summary(model1_corr0.25)

```


```{r Correlation 0.5}
#| include: false

### Assumed correlation value = 0.5 correlation (moderate correlation)
variance.covariance.matrices_0.5 <- 
  make_VCV_matrix(data = data1,
                  V = "effect_size_variance",
                  cluster = "MULTIPLE_ENDPOINT_CLUSTERID",
                  obs = "ACCESSION_ID",
                  type = "vcv",
                  rho = 0.5)   # Setting the assumed correlation value at 0.5
is.positive.definite(variance.covariance.matrices_0.5)

model1_corr0.5 <- rma.mv(yi = effect_size, 
                   V = variance.covariance.matrices_0.5, 
                   random = list(~1 | STUDYID,
                                 ~1 | ACCESSION_ID),
                   data = data1,
                   method = "REML")
summary(model1_corr0.5)

```


```{r Correlation 0.75}
#| include: false

### Assumed correlation value = 0.75 correlation (high correlation)
variance.covariance.matrices_0.75 <- make_VCV_matrix(data = data1,
                                                     V = "effect_size_variance",
                                                     cluster = "MULTIPLE_ENDPOINT_CLUSTERID",
                                                     obs = "ACCESSION_ID",
                                                     type = "vcv",
                                                     rho = 0.75)  # Setting the assumed correlation value at 0.75
is.positive.definite(variance.covariance.matrices_0.75)

model1_corr0.75 <- rma.mv(yi = effect_size,
                          V = variance.covariance.matrices_0.75,
                          random = list(~1 | STUDYID,
                                        ~1 | ACCESSION_ID), 
                   data = data1,
                   method = "REML")
summary(model1_corr0.75)

```

| Assumed correlation | *p*-value                                            | AIC                      |
|---------------------|------------------------------------------------------|--------------------------|
| 0.25 (low)          | `r round(tidy(model1_corr0.25)$p.value, digits = 3)` | `r AIC(model1_corr0.25)` |
| 0.5 (moderate)      | `r round(tidy(model1_corr0.5)$p.value, digits = 3)`  | `r AIC(model1_corr0.5)`  |
| 0.75 (high)         | `r round(tidy(model1_corr0.75)$p.value, digits = 3)` | `r AIC(model1_corr0.75)` |

The low (0.25) and moderate (0.5) correlations have a similar *p*-value and AIC. We decide to use the moderate (0.5) value as most studies do the same (Dougherty et al., 2022).<br>

**NOTE: I DIDN'T REALLY KNOW WHICH CORRELATION OPTION/VALUE TO GO WITH SO JUST PICKED 0.5 AS IT BASICALLY HAD THE JOINT-LOWEST AIC, IT'S WHAT OTHER PEOPLE DO AND IT'S THE DEFAULT CORRELATION VALUE IN THE PACKAGE. LET ME KNOW IF YOU THINK OTHERWISE**<br>

Therefore, the results of our null model are:

```{r Multi-level meta-analysis - null model}
#| echo: false

model1 <- model1_corr0.5

summary(model1)

```

We then quantify the total variance (heterogeneity) attributable to each of the three levels.<br>
Here $I^{2}$ represents the amount of variation not attributable to sampling error. It is split into two parts; within clusters and between-cluster variation.<br>

```{r Testing the heterogeneity}
#| echo: false

i2_ml(model1)

############## NOTE: I'd like to extract values from this if possible, but I'm not sure how to

```

$I^{2}_{Level 2}$ (STUDYID) accounts for ~86% of heterogeneity and $I^{2}_{Level 3}$ (ACCESSION_ID) accounts for ~14% of heterogeneity.<br>
Therefore, this indicates that the majority of variance is attributable to heterogeneity variance within clusters on the second level.<br>


****<br>

Now we calculate the overall estimate of plant growth increase from beetle activity. This is done by back-transforming the lnRR (Bishop & Nakagawa, 2021)<br>

```{r Overall average estimate of plant growth}
#| include: false

################################################################################################################

# NOTE: In this part Bishop & Nakagawa (2021) go: 1 - exp(data). To quote them:
# 'We calculate the average estimate that we report in our paper by back-transforming the lnRR to the scale of ‘yield lost without pollination’ using 1-exp(lnRR).'
# Their code:

#1 - exp(RE0.reml$b[[1]])
## [1] 0.3296884

# But I think for this part in our study it should be exp(data), without the '1 -'
model1.estimate <- 1 - exp(model1$b[[1]])
## [1] -0.1257474
# Beetles reduce yield by 12.5%

################################################################################################################

```

So overall, in the studies we assessed, beetles increased plant growth by `r (round(model1.estimate, 2)*100)`%.<br>

Now we calculate the confidence intervals and prediction intervals.<br>
A prediction interval is the range within which the effect size of a new study would likely be, given this new study was selected at random from the same population of the studies already included in the meta-analysis (Spineli & Pandis, 2020).<br>

```{r Confidence and prediction intervals}
#| echo: false

model1.res <- (mod_results(model = model1,
                           mod = "1",
                           group = "STUDYID",
                           data = data1))
model1.res

# We back-transform the prediction intervals for reporting in the paper.
model1.lowerPR <- 1 - exp(model1.res$mod_table$lowerPR)
model1.upperPR <- 1 - exp(model1.res$mod_table$upperPR)
# We back-transform the confidence intervals for reporting in the paper.
model1.lowerCL <- 1 - exp(model1.res$mod_table$lowerCL)
model1.upperCL <- 1 - exp(model1.res$mod_table$upperCL)

```


# Visualise the results

We can now make an orchard plot to visualise our results.<br>
The orchard plot contains confidence intervals (thick horizontal lines) and prediction intervals (thin horizontal lines) around a point estimate.<br>
The points represent scaled individual effect sizes.<br>
See Nakagawa et al. (2021) for more information on orchard plots.<br>

```{r Orchard plot}
#| echo: false
#| message: false

wes_colours <- wes_palettes$GrandBudapest1 # prepping the colours for later on

figure_orchard <- 
orchard_plot(model1, 
             mod = "1",   # Set to "1" for intercept only model
             group = "STUDYID", 
             data = data1, 
             #k = FALSE,   # Mute the 'k = (number of effect sizes)' label
             xlab = "lnRR (effect size)") +
  scale_fill_manual(values=wes_colours[2]) +
  scale_colour_manual(values=wes_colours[3]) +  
  theme(panel.background = element_rect(fill = "transparent", 
                                        color = NA),   # background of the panel
        plot.background = element_rect(fill = "transparent", 
                                       color = NA),   # background of the plot
        legend.background = element_rect(fill = "transparent"),    # get rid of legend background
        legend.key = element_rect(fill = "transparent", 
                                  colour = NA))    # get rid of legend background
figure_orchard

# The orachaRd package/paper explainer has some good code for modifying orchard plots (https://github.com/daniel1noble/orchaRd) (Nakagawa et al., 2021)

ggsave(filename = (here("figures/orchard_plot.png")),
       plot = figure_orchard,
       bg = "transparent")   # Plot with a transparent background

# If there is a 'Graphics API version mismatch' when using ggsave try re-installing the ragg package

```


We can also make a caterpillar plot.<br>
The caterpillar plot shows effect sizes for each outcome along with their confidence intervals.
At the bottom it also shows a diamond that represents an overall mean along with confidence intervals (mean in the centre and CIs are the horizontal points of the diamond). This diamond is within the prediction intervals.<br>
See Nakagawa et al. (2021) for more information on caterpillar plots.<br>

```{r Caterpillar plot 1}
#| echo: false

figure_caterpillar <- 
caterpillars(model1.res, 
             mod = "1", 
             group = "STUDYID", 
             data = data1,
             #k = FALSE,
             xlab = "effect size") +
  theme_classic() +
  scale_fill_manual(values=wes_colours) +
  scale_colour_manual(values=wes_colours) +  
  theme(panel.background = element_rect(fill = "transparent", 
                                        color = NA),
        plot.background = element_rect(fill = "transparent", 
                                       color = NA),
        legend.background = element_rect(fill = "transparent"),
        legend.key = element_rect(fill = "transparent", 
                                  colour = NA))
figure_caterpillar

ggsave(filename = (here("figures/caterpillar_plot.png")),
       plot = figure_caterpillar,
       bg = "transparent")

```

**NOTE: Here's my attempt at a caterpillar plot. I imagine that the caterpillar plot will go in powerpoint presentations so I thought I'd make one that is customisable. My one looks similar, however, the horizontal bars look different. I think this is because I have used effect size variance for each effect size, but I should be using confidence intervals. I'm not too sure of how to extract the confidence intervals though. It might not be worth pursuing though** (<https://wviechtb.github.io/metafor/reference/confint.rma.html>)

```{r Assigning confidence intervals for study outcomes}
#| include: false

# HERE I THINK I NEED TO CALCULATE THE CONFIDENCE INTERVALS FOR EACH INDIVIDUAL STUDY OUTCOME
# INSTEAD I HAVE USED THE 'effect_size_variance' WHICH I THINK IS INCORRECT
data1$lowerCI <- data1$effect_size - data1$effect_size_variance   # Effect size - 'effect_size_variance' (Should be confidence intervals??)
data1$upperCI <- data1$effect_size + data1$effect_size_variance   # Effect size + 'effect_size_variance' (Should be confidence intervals??)

```

```{r}
#| include: false

################################################################################

# Below is a failed attempt to calculate confidence intervals for the individual study outcomes 
# The code would probably work for a Standardised Mean Difference effect size, but doesn't seem to work for a log response effect size

################################################################################


## From this website https://www.uvm.edu/~statdhtx/methods8/Supplements/MISC/Confidence%20Intervals%20on%20Effects/Effect%20Sizes%20Confidence%20Intervals.html

# Calculate the confidence intervals for each study outcome

# data1$pooled_variance <- (((data1$TREATMENT_N - 1) * data1$TREATMENT_SD) + ((data1$CONTROL_N - 1) * data1$CONTROL_SD)) /
#   (data1$TREATMENT_N + data1$CONTROL_N - 2)
# data1$pooled_variance
# 
# data1$critical_value <-
#   qt(p= 0.05/2, 
#      df = (data1$TREATMENT_N + data1$CONTROL_N - 2), 
#      lower.tail=FALSE)
# data1$critical_value
# 
# data1$upperCI <- (data1$TREATMENT_MEAN - data1$CONTROL_MEAN) + data1$critical_value * (sqrt ((data1$pooled_variance/data1$TREATMENT_N) + (data1$pooled_variance/data1$CONTROL_N)))
# data1$lowerCI <- (data1$TREATMENT_MEAN - data1$CONTROL_MEAN) - data1$critical_value * (sqrt ((data1$pooled_variance/data1$TREATMENT_N) + (data1$pooled_variance/data1$CONTROL_N)))
# 
# data1$effect_size_me <- (data1$TREATMENT_MEAN - data1$CONTROL_MEAN)/sqrt(data1$pooled_variance)
# data1$effect_size_me
# data1$effect_size
# 
# 
# data1$effect_size_me2 <- log(data1$TREATMENT_N/data1$CONTROL_N)

```

```{r Caterpillar plot 2}
#| echo: false

# Data manipulation to enable geom_path() to plot confidence intervals (grouping them by ACCESSION_ID)
data1_long <-   # Subset of data1 with the columns 'data1, ACCESSION_ID, lowerCI, upperCI, effect_size'
  select(data1, ACCESSION_ID, lowerCI, upperCI, effect_size) 
data1_long <- data1_long[order(data1_long$effect_size), ]   # Sort dataset by effect size to make caterpillar plot look proper
data1_long$y_coord <- nrow(data1_long):1   # Create a list of numbers that will later act as y-coordinates in the plot

data1_long <-
  data1_long %>%
  pivot_longer(
    cols = ends_with("CI"),
    names_to = "CI"
  )

figure_caterpillar2 <- 
  ggplot() +
# Insert a vertical line at 0 indicating 'no effect'
  geom_vline(xintercept = 0,   # Set the position along the x-axis for the vertical line 
             colour = "grey",   # Set the line colour
             linetype = "longdash") +   # Make the line dashed
# Insert confidence intervals for each individual study outcome
geom_path(data = data1_long,
                           aes(x = value,   # Set the x-coordinate
                               y = y_coord,   # Set the y-coordinate
                               group = ACCESSION_ID),   # Group the coordinates (as each line on the plot is made from x- and y-coodinates on seperate lines in the dataset)
          colour = "dark grey",   # Set the line colour
          size = 1,   # Set the line size
          lineend = "round") +   # Make the ends of the line round (rather than the default square)
# Insert the effect size for each individual study outcome
geom_point(data = data1_long,
           aes(x = effect_size,   # Set the effect size
               y = y_coord),   # Set the y-coordinate
           col = "black",   # Set the point colour
           size = 1) +   # Set the point size
# Adding plot aesthetics
  theme_classic() +   # Make the plot background blank   
  theme(panel.background = element_rect(fill = "transparent"),   # Make the panel background transparent   
        plot.background = element_rect(fill = "transparent",   # Make the plot background transparent   
                                       color = NA),
        axis.text.y=element_blank(),   # Mute the y-axis text
        axis.ticks.y=element_blank()) +   # Mute the y-axis ticks
  labs(title = "Caterpillar plot",   # Label the plot
       x ="lnRR (effect size)",   # Label the x-axis (using 'expression' and 'paste' to incorporate an italic letter)
       y = NULL)   # Mute the y-axis label


# Set how far off the y-axis the diamond will be 
summary_bar_spacer <- 10


id <- "a"   # Create an arbitrary id for the upcoming diamond
x_coordinates <- c(model1.lowerCL,   # Lower confidence interval for the overall study (left point of diamond)
                   model1.estimate,   # Mean effect size for the overall study (horizontal mid-point of diamond) 
                   model1.upperCL,   # Upper confidence interval for the overall study (right point of diamond)
                   model1.estimate)   # Mean effect size for the overall study (horizontal mid-point of diamond)  
y_coordinates <- c(summary_bar_spacer,   # Vertical mid-point of the diamond (equal to summary_bar_spacer)
                   summary_bar_spacer+2,   # Top point of the diamond (equal to summary_bar_spacer + 2) 
                   summary_bar_spacer,   # Vertical mid-point of the diamond (equal to summary_bar_spacer)
                   summary_bar_spacer-2)   # Bottom point of the diamond (equal to summary_bar_spacer - 2) 
plot.diamond <- data.frame(id, x_coordinates, y_coordinates)   # Make a dataframe of the diamond coordinates


# Now add in the diamond and prediction intervals (it will be underneath the diamond)
figure_caterpillar2 <- figure_caterpillar2 +
  # Add the overall prediction intervals to the plot
  geom_linerange(data = plot.diamond,
                 aes(x = model1.estimate,
                     y = summary_bar_spacer,
                     #y = 1,
                     xmax = model1.upperPR,   # The lower prediction interval
                     xmin = model1.lowerPR),   # The upper prediction interval
                 col = "grey",
                 size = 1) +
  
# Add the diamond to the caterpillar plot (it mill now be on top of the line for the overall prediction intervals)
    geom_polygon(data = plot.diamond,   # Using the dataframe with the diamond coordinates
               aes(fill = id,   # Fill diamond according to the (arbitrary) id (this is kind of meaningless when creating a single shape)
                   group = id,   # Group coordinates according to the (arbitrary) id (this is kind of meaningless when creating a single shape)
                   x = x_coordinates,   # Plot x-coordinates
                   y = y_coordinates),   # Plot y-coordinates
               fill = "black",   # Colour the centre of the diamond
               col = "black")   # Colour the edge of the diamond
figure_caterpillar2


ggsave(filename = (here("figures/caterpillar_plot2.png")),
       plot = figure_caterpillar2,
       bg = "transparent")

```

# Investigating moderators

Next we tested to see if moderators improved the model fit.<br> 

**NOTE:**
**Below I've looked at a few moderators, but I'm mainly concerned with the ecological moderators looking at the beetle community:**<br>
**Beetle abundance**<br>
**Beetle biomass**<br>
**Beetle species richness**<br>
**Beetle functional group**<br>

**To a lesser extent I'm interested in the experimental moderators looking at:**<br>
**Plant clade**<br>
**Had the plants germinated before dung was added**<br>
**Dung quantity per beetle/surface area**<br>

**So in the final code/analysis I can probably drop some of those moderators**<br>

## Categorical moderators:

The results from testing moderators:<br>

We tested moderators individually by adding them to our null model (model1), and comparing model performance with and without the moderator.<br>

**NOTE: These were all tested with an assumed correlation (rho) of 0.5. Also, I don't really know which of the models to go with or if any of the results are that meaningful. I've noted the points that look interesting to me, but I'm likely wrong**<br>


## Beetle functional group

Firstly, with no moderator

```{r Beetle functional group - no moderator}
#| echo: false
#| warning: false

## Model 1 (no moderator):
variance.covariance.matrices_0.5 <- make_VCV_matrix(data = data1, 
                                                    V = "effect_size_variance",
                                                    cluster = "STUDYID",
                                                    obs = "ACCESSION_ID",
                                                    type = "vcv",
                                                    rho = 0.5)   # Setting the assumed correlation value at 0.5

model1 <- rma.mv(yi = effect_size,
                 V = variance.covariance.matrices_0.5,
                 random = list(~1 | STUDYID,
                               ~1 | ACCESSION_ID),
                 data = data1,
                 method = "REML")
summary(model1)
#tidy(model1)

```

Secondly, with a moderator

```{r Beetle functional group - with a moderator}
#| echo: false
#| warning: false

## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~FUNCTIONAL_GROUP, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)

```

Finally, with a moderator (and the intercept removed)

```{r Beetle functional group - with a moderator and intercept removed}
#| echo: false
#| warning: false

## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~FUNCTIONAL_GROUP - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)

model3.res <- (mod_results(model3,
                           group = "STUDYID",
                           data = data1,
                           mod = "FUNCTIONAL_GROUP"))

```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE: I've plotted model 3 below, but Model 1 has a lower AIC value which would mean it's a better fit? It seems like rollers may be improving plant growth?**

Orchard plot - with a moderator (and the intercept removed)

```{r Plotting Beetle functional group}
#| echo: false

FUNCTIONAL_GROUP.plot <- 
  orchard_plot(model3.res,
               mod = "1",
               group = "STUDYID",
               data = data1,
               xlab = "lnRR (effect size)") +
    labs(title = "Beetle functional group influencing plant growth",
         x = "Beetle functional group")   # The x-axis and y-axis labels are counter-intuitive in this plot
FUNCTIONAL_GROUP.plot

```

## Plant clade

Firstly, with no moderator

```{r Plant clade - no moderator}
#| echo: false
#| warning: false

## Model 1 (no moderator):
summary(model1)
#tidy(model1)

```

Secondly, with a moderator

```{r Plant clade - with a moderator}
#| echo: false
#| warning: false

## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~CLADE, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)

```

Finally, with a moderator (and the intercept removed)

```{r Plant clade - with a moderator and intercept removed}
#| echo: false
#| warning: false

## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~CLADE - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)

model3.res <- (mod_results(model3,
                           group = "STUDYID",
                           data = data1,
                           mod = "CLADE"))

```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE: I've plotted model 3 below, but Model 1 has a lower AIC value which would mean it's a better fit? Also, monocots have the biggest increase in plant growth? Seems like dicots drag the effect size down**

Orchard plot - with a moderator (and the intercept removed)

```{r Plotting Plant clade}
#| echo: false

CLADE.plot <- 
  orchard_plot(model3.res,
               mod = "1",
               group = "STUDYID",
               data = data1,
               xlab ="lnRR (effect size)") +
    labs(title = "Plant clade influencing plant growth",
         x = "Plant clade")
CLADE.plot

```


## Had the plants germinated before dung was added

Firstly, with no moderator

```{r Had the plants germinated before dung was added - no moderator}
#| echo: false
#| warning: false

## Model 1 (no moderator):
summary(model1)
#tidy(model1)

```

Secondly, with a moderator

```{r Had the plants germinated before dung was added - with a moderator}
#| echo: false
#| warning: false

## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~PLANTS_GERMINATED_BEFORE_DUNG_ADDITION, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)

```

Finally, with a moderator (and the intercept removed)

```{r Had the plants germinated before dung was added - with a moderator and intercept removed}
#| echo: false
#| warning: false

## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~PLANTS_GERMINATED_BEFORE_DUNG_ADDITION - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)

model3.res <- (mod_results(model3,
                           group = "STUDYID",
                           data = data1,
                           mod = "PLANTS_GERMINATED_BEFORE_DUNG_ADDITION"))

```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE: Model 1 is the best fit. Plants being established prior to dung addition is not influential to plant growth?**

Orchard plot - with a moderator (and the intercept removed)

```{r Plotting Had the plants germinated before dung was added}
#| echo: false

PLANTS_GERMINATED_BEFORE_DUNG_ADDITION.plot <- 
  orchard_plot(model3.res,
               mod = "1",
               group = "STUDYID",
               data = data1,
               xlab ="lnRR (effect size)") +
    labs(title = "Plant germination time influencing plant growth",
         x = "Plant germinated before dung addition")
PLANTS_GERMINATED_BEFORE_DUNG_ADDITION.plot

```


# Numerical moderators:

## Number of beetles

Firstly, with no moderator

```{r Number of beetles - no moderator}
#| echo: false
#| warning: false

## Model 1 (no moderator):
summary(model1)
#tidy(model1)

```

Secondly, with a moderator

```{r Number of beetles - with a moderator}
#| echo: false
#| warning: false

## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~NUMBER_OF_BEETLES, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)

```

Finally, with a moderator (and the intercept removed)

```{r Number of beetles - with a moderator and intercept removed}
#| echo: false
#| warning: false

## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~NUMBER_OF_BEETLES - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)

```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE 1: I think model 1 is the best fit, but maybe there are some outliers here to be removed and then re-analyse?**<br>

Visualise data

**NOTE 2: I should make a trendline for the plot using the predict function, however I struggle a little with implementing predict and it will take me time. Therefore, I've left the trendlines out for now, but if these numerical moderator plots end up in the final analysis I'll spend the time to get them working**<br>

```{r Plotting  Number of beetles}
#| echo: false
#| warning: false

NUMBER_OF_BEETLES.plot <- 
  ggplot(data = data1, 
         aes(x = NUMBER_OF_BEETLES,
             y = effect_size)) +
  geom_point(aes(size = precision),   # Point size corresponds to effect size variance
             shape = 21,   # Shape 21 allows for a different coloured ring around each point
             fill = NA,   # Remove the fill colour of the points
             color = "black") +   # Make the outer ring of the points black
  labs(title = "Number of beetles influencing plant growth",   # Lab title 
       x = "Number of beetles",   # X-axis label
       y ="lnRR (effect size)",   # Y-axis label - using an expression to insert an italic letter
       size = "Precision\n1/SE") +
  theme_classic() +
  theme(panel.background = element_rect(fill = "transparent"),   # bg of the panel
        plot.background = element_rect(fill = "transparent", 
                                       color = NA))
NUMBER_OF_BEETLES.plot

```


## Total biomass of beetles

Firstly, with no moderator

```{r Total biomass of beetles - no moderator}
#| echo: false
#| warning: false

## Model 1 (no moderator):
summary(model1)
#tidy(model1)

```

Secondly, with a moderator

```{r Total biomass of beetles - with a moderator}
#| echo: false
#| warning: false

## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~TOTAL_BEETLE_WEIGHT_IN_MG, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)

```

Finally, with a moderator (and the intercept removed)

```{r Total biomass of beetles - with a moderator and intercept removed}
#| echo: false
#| warning: false


## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~TOTAL_BEETLE_WEIGHT_IN_MG - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)

```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE 1: I think model 1 is the best fit, but maybe there are some outliers here to be removed and then re-analyse?**<br>

Visualise data

```{r Plotting Total biomass of beetles}
#| echo: false
#| warning: false

TOTAL_BEETLE_WEIGHT_IN_MG.plot <- 
  ggplot(data = data1, 
         aes(x = TOTAL_BEETLE_WEIGHT_IN_MG,
             y = effect_size)) +
  geom_point(aes(size = precision),
             shape = 21,
             fill = NA,
             color = "black") +
  labs(title = "Total beetle biomass influencing plant growth",
       x = "Total beetle biomass (mg)",
       y ="lnRR (effect size)",
       size = "Precision\n1/SE") +
  theme_classic() +
  theme(panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent",
                                       color = NA))
TOTAL_BEETLE_WEIGHT_IN_MG.plot

```


## Beetle species richness

Firstly, with no moderator

```{r Beetle species richness - no moderator}
#| echo: false
#| warning: false

## Model 1 (no moderator):
summary(model1)
#tidy(model1)

```

Secondly, with a moderator

```{r Beetle species richness - with a moderator}
#| echo: false
#| warning: false

## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~SPECIES_RICHNESS, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)

```

Finally, with a moderator (and the intercept removed)

```{r Beetle species richness - with a moderator and intercept removed}
#| echo: false
#| warning: false

## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~SPECIES_RICHNESS - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)

model3.res <- (mod_results(model3,
                           group = "STUDYID",
                           data = data1,
                           mod = "SPECIES_RICHNESS"))

```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE: I've plotted model 3 below, but Model 1 has a lower AIC value which would mean it's a better fit? It seems like plant growth may be improving when 4 beetles are present?**

Orchard plot - with a moderator (and the intercept removed)

```{r Plotting Beetle species richness 1}
#| echo: false

SPECIES_RICHNESS.plot <- 
  ggplot(data = data1, 
         aes(x = SPECIES_RICHNESS,
             y = effect_size)) +
  geom_point(aes(size = precision),
             shape = 21,
             fill = NA,
             color = "black") +
  labs(title = "Species richness influencing plant growth",
       x = "Species richness",
       y ="lnRR (effect size)",
       size = "Precision\n1/SE") +
  theme_classic() +
  theme(panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent",
                                       color = NA))
SPECIES_RICHNESS.plot

```

Visualise data

```{r Plotting Beetle species richness 2}
#| echo: false

SPECIES_RICHNESS.plot2 <- 
  orchard_plot(model3.res,
               mod = "1",
               group = "STUDYID",
               data = data1,
               xlab ="lnRR (effect size)") +
    labs(title = "Species richness influencing plant growth",
         x = "Species richness")                                                # This x label actually corresponds to the y axis
SPECIES_RICHNESS.plot2

```


## Amount of dung per surface area

Firstly, with no moderator

```{r Amount of dung per surface area - no moderator}
#| echo: false
#| warning: false

## Model 1 (no moderator):
summary(model1)
#tidy(model1)

```

Secondly, with a moderator

```{r Amount of dung per surface area - with a moderator}
#| echo: false
#| warning: false

## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~GRAMS_DUNG_PER_CM_SQUARED, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)

```

Finally, with a moderator (and the intercept removed)

```{r Amount of dung per surface area - with a moderator and intercept removed}
#| echo: false
#| warning: false

## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~GRAMS_DUNG_PER_CM_SQUARED - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)

```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE: ^ Model 2 looks best here? Although maybe it would be best to remove that study with the high grams per dung**


```{r Plotting Amount of dung per surface area}
#| echo: false
#| warning: false

GRAMS_DUNG_PER_CM_SQUARED.plot <- 
  ggplot(data = data1, 
         aes(x = GRAMS_DUNG_PER_CM_SQUARED,
             y = effect_size)) +
  geom_point(aes(size = precision),
             shape = 21,
             fill = NA,
             color = "black") +
  labs(title = "Dung quantity influencing plant growth",
       x = "Dung quantity (grams per cm squared)",
       y ="lnRR (effect size)",
       size = "Precision\n1/SE") +
  theme_classic() +
  theme(panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent",
                                       color = NA))
GRAMS_DUNG_PER_CM_SQUARED.plot

```


## Amount of dung per beetle

Firstly, with no moderator

```{r Amount of dung per beetle - no moderator}
#| echo: false
#| warning: false

## Model 1 (no moderator):
summary(model1)
#tidy(model1)

```

Secondly, with a moderator

```{r Amount of dung per beetle - with a moderator}
#| echo: false
#| warning: false

## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~GRAMS_DUNG_PER_BEETLE, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)

```

Finally, with a moderator (and the intercept removed)

```{r Amount of dung per beetle - with a moderator and intercept removed}
#| echo: false
#| warning: false

## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~GRAMS_DUNG_PER_BEETLE - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)

```

Visualise data

```{r Plotting Amount of dung per beetle}
#| echo: false
#| warning: false

GRAMS_DUNG_PER_BEETLE.plot <- 
  ggplot(data = data1, 
         aes(x = GRAMS_DUNG_PER_BEETLE,
             y = effect_size)) +
  geom_point(aes(size = precision),
             shape = 21,
             fill = NA,
             color = "black") +
  labs(title = "Dung quantity influencing plant growth",
       x = "Dung quantity (grams per beetle)",
       y ="lnRR (effect size)",
       size = "Precision\n1/SE") +
  theme_classic() +
  theme(panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent",
                                       color = NA))
GRAMS_DUNG_PER_BEETLE.plot

```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE: Model 1 seems to be the best fit**

**NOTE: Overall I don't think the moderators investigated are that influential**


## Publication bias

Next we check for publication bias and it's recommended to do this with more than one source (Nakagawa et al., 2017).<br>

Firstly we produce a funnel plot, and as the moderators were not influential these are omitted.<br>

Nakagawa & Lagisz (2016) give brief instructions on interpreting a funnel plot.


```{r Funnel plot without moderators}
#| echo: false
#| warning: false


png(file = (here("figures/funnel_plot.png")))

figure_funnel <-
funnel(model1,
       vi = variance.covariance.matrices_0.5,
       yaxis = "seinv",   # The measure of precision for the y-axis
       main = "Model 1 - Doesn't include moderators",   # Main title
       legend = TRUE,
       back	= "transparent",   # colour to use for the background of the plotting region
       hlines = "lightgray",   # colour of the horizontal reference lines
       col = "black",   # specify point colour
       bg = "transparent")   # specify the background colour for the plot
figure_funnel

dev.off()

# This code chunk only seems to work on the first execution for some reason

```

**NOTE: I might have to trim this down as it looks a bit weird, there seems to be no skew though**<br>
**NOTE: This figure has been troublesome to knit**<br>

As a second assessment of publication bias, we use Egger's regression (see Egger et al. (1997)), with the standard
error, SE (sqrt(effect_size_variance)) as a moderator. A significant slope for SE indicates statistically significant funnel asymmetry after controlling for all other variables in the model (Bishop & Nakagawa, 2021).<br>

```{r Eggers test}
#| echo: false
#| warning: false

# sqrt(vi) = sampling standard error
# test is if sqrt(vi) moderator is statistically sig = asymmetry in funnel plot

egger <- rma.mv(yi = effect_size,
                V = variance.covariance.matrices_0.5,
                mods = sqrt(effect_size_variance),
                random = list(~1 | STUDYID,
                              ~1 | ACCESSION_ID),
                data = data1,
                method = "REML")
summary(egger)

```

The results indicate funnel symmetry. Now to visualise the Egger's regression.

```{r visualise Eggers test}
#| echo: false
#| warning: false

# A bubble plot showing a predicted loess line (dashed black line) for the contentious variable ‘sqrt(effect_size_variance)‘, with their 95% confidence
# regions (orange dotted lines) and 95% prediction regions (blue dotted lines) with observed effect sizes based on
# various sample sizes. Note that the lines are not linear as these are based on multivariate predictions of the data points.

egger_predict <- predict.rma(egger)


figure_eggers <- data1 %>% 
  mutate(ymin = egger_predict$ci.lb, 
         ymax = egger_predict$ci.ub,
         ymin2 = egger_predict$cr.lb, 
         ymax2 = egger_predict$cr.ub, 
         pred = egger_predict$pred) %>%
ggplot(aes(x = sqrt(effect_size_variance), 
           y = effect_size, size = sqrt(1/effect_size_variance))) + 
  geom_point(shape = 21,
             fill = (values=wes_colours[2])) + 
  geom_smooth(aes(y = ymin2), 
              method = "loess",
              se = FALSE, 
              lty = "dotted", 
              lwd = 0.75, 
              colour = "#0072B2") +
  geom_smooth(aes(y = ymax2), 
              method = "loess", 
              se = FALSE,
              lty = "dotted", 
              lwd = 0.75, 
              colour = "#0072B2") + 
  geom_smooth(aes(y = ymin),
              method = "loess", 
              se = FALSE, 
              lty = "dotted", 
              lwd = 0.75,
              colour = "#D55E00") + 
  geom_smooth(aes(y = ymax), 
              method = "loess",
              se = FALSE, 
              lty = "dotted", 
              lwd = 0.75, 
              colour = "#D55E00") +
  geom_smooth(aes(y = pred), 
              method = "loess", 
              se = FALSE,
              lty = "dashed", 
              lwd = 1, 
              colour = "black") + 
  labs(x = "sqrt(sampling variance)",
       y = "lnRR (effect size)", 
       size = "Precision (1/SE)") + 
  guides(fill = "none",
         colour = "none") + # themes
  theme_bw()+  
  theme(panel.background = element_rect(fill = "transparent", 
                                        color = NA),
        plot.background = element_rect(fill = "transparent", 
                                       color = NA),
        legend.background = element_rect(fill = "transparent"),
        legend.key = element_rect(fill = "transparent", 
                                  colour = NA))

figure_eggers

ggsave(filename = (here("figures/eggers_plot.png")),
       plot = figure_eggers,
       bg = "transparent")

```


**NOTE: It might be possible to conduct an un-weighted meta-analysis. It's probably unnecessary given the funnel plot and eggers test, but could be a good way to incorporate those studies that were excluded from the analysis for not having standard deviations/errors**


## Un-weighted meta-analysis

14 reports/studies were included in this meta-analysis, but there were an additional 10 reports/studies that lacked the full information needed to calculate a SMD (mean, SD & sample size).<br>

``` {r}
#| include: false

# Trying an un-weighted meta-analysis
# Nakagawa et al., 2017: 
# 'a lack of the information required to obtain sampling variance for a portion of the dataset (for example, missing standard deviations)' ...[to deal with this]... 'the authors should run models both with and without data with sampling variance information; note that without sampling variance (that is, unweighted meta-analysis) the analysis becomes a normal linear model (Morrissey, 2016)'

# Osburn & Callender (1992) have a formula for an unweighted meta-analysis.
# I think an un-weighted meta-analysis is also called an 'informal' or a 'bare-bones' meta-analysis

# Information also here: https://www.metafor-project.org/doku.php/analyses:miller1978?s[]=unweighted

model1

res <- rma(yi = effect_size, 
           vi = effect_size_variance, 
           method="EE", 
           data=data2, 
           weighted=FALSE)   # Specify an un-weighted meta-analysis
res

funnel(res)

```


## References:
Bishop, J., & Nakagawa, S. (2021). Quantifying crop pollinator dependence and its heterogeneity using multi-level meta-analysis. Journal of Applied Ecology, 58(5), 1030–1042. https://doi.org/10.1111/1365-2664.13830<br>
Dougherty, L. R., Skirrow, M. J. A., Jennions, M. D., & Simmons, L. W. (2022). Male alternative reproductive tactics and sperm competition: a meta-analysis. Biological Reviews, 97, 1365–1388. https://doi.org/10.1111/brv.12846<br>
Hedges, L. V., Gurevitch, J., & Curtis., P. S. (2010). The meta‐analysis of response ratios in experimental ecology. Global Change Biology, 16(3), 1046–1056. https://doi.org/10.1890/0012-9658(1999)080[1150:TMAORR]2.0.CO;2<br>
Johnson, S. N., Lopaticki, G., Barnett, K., Facey, S. L., Powell, J. R., & Hartley, S. E. (2016). An insect ecosystem engineer alleviates drought stress in plants without increasing plant susceptibility to an above-ground herbivore. Functional Ecology, 30(6), 894–902. https://doi.org/10.1111/1365-2435.12582<br>
Manning, P., Slade, E. M., Beynon, S. A., & Lewis, O. T. (2017). Effect of dung beetle species richness and chemical perturbation on multiple ecosystem functions. Ecological Entomology, 42(5), 577–586. https://doi.org/10.1111/een.12421<br>
Miranda, C. H. B., Santos, J. C. C., & Bianchin, I. (1998). Contribuição de Onthophagus gazella à melhoria da fertilidade do solo pelo enterrio de massa fecal bovina fresca. Brazilian Journal of Animal Science, 27, 681–685.<br>
Nakagawa, S., & Lagisz, M. (2016). Visualizing unbiased and biased unweighted meta-analyses. Journal of Evolutionary Biology, 29(10), 1914–1916. https://doi.org/10.1111/jeb.12945<br>
Nakagawa, Shinichi, & Santos, E. S. A. (2012). Methodological issues and advances in biological meta-analysis. Evolutionary Ecology, 26(5), 1253–1274. https://doi.org/10.1007/s10682-012-9555-5<br>
Nervo, B., Caprio, E., Celi, L., Lonati, M., Lombardi, G., Falsone, G., Iussig, G., Palestrini, C., Said-Pullicino, D., & Rolando, A. (2017). Ecological functions provided by dung beetles are interlinked across space and time: Evidence from 15N isotope tracing. Ecology, 98(2), 433–446. https://doi.org/10.1002/ecy.1653<br>
Noble, D. W. A., Lagisz, M., O’dea, R. E., & Nakagawa, S. (2017). Nonindependence and sensitivity analyses in ecological and evolutionary meta‐analyses. Molecular Ecology, 26, 2410–2425. https://doi.org/doi.org/10.1111/mec.14031<br>
Page, M. J., McKenzie, J. E., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., Shamseer, L., Tetzlaff, J. M., Akl, E. A., Brennan, S. E., Chou, R., Glanville, J., Grimshaw, J. M., Hróbjartsson, A., Lalu, M. M., Li, T., Loder, E. W., Mayo-Wilson, E., McDonald, S., … Moher, D. (2021). The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. The BMJ, 372. https://doi.org/10.1136/bmj.n71<br>
Spineli, L. M., & Pandis, N. (2020). Prediction interval in random-effects meta-analysis. American Journal of Orthodontics and Dentofacial Orthopedics, 157(4), 586–588. https://doi.org/10.1016/j.ajodo.2019.12.011<br>
Viechtbauer, W. (2010). Conducting meta-analyses in {R} with the {metafor} package. Journal of Statistical Software, 36(3), 1–48. https://doi.org/10.18637/jss.v036.i03<br>
