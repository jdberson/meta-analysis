---
output: html_document
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 80
---

```{=html}
<style type="text/css">
.main-container {
  max-width: 800px;
  margin-left: auto;
  margin-right: auto;
}
</style>
```

### NOTES:<br>
**I may be able to impute standard deviations for the studies that don't have them, as was done in Bishop & Nakagawa (2021). I found that bit tricky though when testing it out**<br>
**It would be good to use the renv package, but I'll wait until this analysis/code is sorted before adding in that element**<br>
**I'm happy to use Quarto if you think that would be better than Markdown**<br>
**Do I eventually shorten colnames**

# Dung beetles increase plant growth: a meta-analysis

Our two aims with this project are to conduct a systematic review and then meta-analysis to assess:

## 1) When dung beetles access dung, does plant growth increase in the immediate area?

## 2) Are there ecological and experimental factors (moderators) that influence this plant growth?
<br>

```{r Load libraries and set parent directory}
#| include: false
#| message: false
#| warning: false
# Load libraries
library("broom")
library("dmetar")
library("dplyr")
library("here")
library("knitr")
library("matrixcalc")
library("metaAidR")
library("metafor")
library("multcompView")
library("orchaRd")   # github.com/itchyshin/orchard_plot
library("readxl")
library("tidyverse")
library("wesanderson")
# Set the parent directory so that the relative paths used later work properly
here()   
```

Firstly we conducted a systematic review following the PRISMA 2020 method (Page et al., 2021).<br>
<br>

```{r}
#| echo: false
#| out.width: '100%'

#include_graphics(here("figures/prisma_flow_diagram.png"))   # Insert the PRISMA Flow Diagram using knitr
include_graphics("figures/prisma_flow_diagram.png")

```

We then extracted data from those reports to conduct a meta-analysis.<br>

```{r Import data}
#| include: false
data <- read_excel(here("data/meta_analysis_data.xlsx"),
                   na = c("", "NA"))   # Import dataset from the data folder

```


```{r Prep data for analysis}
#| include: false
# Turn the required columns into factors:
data <-
data %>%
  mutate(across(.cols = c(STUDYID,
                          ACCESSION_ID,
                          MULTIPLE_ENDPOINT_CLUSTERID,
                          SHAREDCONTROL_CLUSTERID,
                          TREATMENT,
                          FUNCTIONAL_GROUP,
                          SPECIES_RICHNESS,
                          BEETLES_CONFINED_TO_CAGE,
                          DUNG_TYPE,
                          DUNG_FROZEN_PRIOR_TO_ALIQUOTTING,
                          DUNG_REMOVED_AFTER_BEETLE_ACTIVITY,
                          PLANTS_GERMINATED_BEFORE_DUNG_ADDITION,
                          PLANT_TYPE,
                          CLADE,
                          RESPONSE_AREA,
                          RESPONSE_MEASUREMENT,
                          RESPONSE,
                          CONTROL_TREATMENT,
                          EXPERIMENT_SETTING), 
                as.factor))
sapply(data, class)   # Check that the required columns are now factors

data <-
  data %>%
  mutate(GRAMS_DUNG_PER_CM_SQUARED = DUNG_QUANTITY_IN_GRAMS / SURFACE_AREA_CM_SQUARED,   # Calculate a variable for the grams of dung per cm squared of each plot/mesocosm
         GRAMS_DUNG_PER_BEETLE = DUNG_QUANTITY_IN_GRAMS / SURFACE_AREA_CM_SQUARED)   # Calculate a variable for the grams of dung per beetle

```

Our dataset features correlated, or dependent, outcomes within studies (Noble et al., 2017). This correlation can be due to:<br> 
**Shared control** - Where multiple treatments are compared to the same control (e.g. Manning et al., 2017).<br>
**Shared measurements** - Where more than one outcome/measurement is reported for the same study (e.g. Johnson et al., 2016).<br>
**Within-study temporal correlation** - Where the effect is measured at multiple timepoints (e.g. Nervo et al., 2017).<br>
For more information on non-dependent effect sizes see Nakagawa & Santos (2012), Noble et al. (2017) and Bishop & Nakagawa (2021).<br>
<br>
We deal with the **shared control** aspect of our dataset before calculating the effect sizes.<br>
<br>
We now modify the control sample size (CONTROL_N) to account for those studies with shared controls (Bishop and Nagakawa, 2021). **I think the citation should be Higgins et al. (2019), you will need to check this. There are a few instances where Bishop and Nakagawa are cited where I think it should be the original study. I'd suggest for the paper / supp material to have an overall statement that R code was informed by / benefited from code provided Bishop and Nakagawa in their supp material, and then cite the relevant studies throughout.**
To do this we divide the sample size of each control (CONTROL_N) by the number of shared controls within its study (SHAREDCONTROL_COUNT). 
This creates a SHARED_CONTROL_N value which is then used (in escalc) to calculate the effect size for each outcome.

We then use measures of plant growth between 'dung+beetles' and 'dung only' treatments to calculate effect sizes. Effect sizes were calculated using a log transformed ratio of means, or log response ratio (lnRR) (Hedges et al., 1999). This is a usefull measurement as we can determine a percentage increase in plant growth. The metafor package was used to calculate the effect sizes (Viechtbauer 2010). For measure = "ROM", the log is taken of the ratio of means, which makes this outcome measure symmetric around 0 and yields a corresponding sampling distribution that is closer to normality (Viechtbauer 2010).<br> 

**Jake's addition**

I noticed in the comments in the next code chunk the dung_only in treatment and soil_only in control so I performed the following checks to explore the data a little more. The code chunk should be deleted.

Some questions that came up:

Was there a plan to compare dung_only vs soil_only (i.e. test for a dung effect)? 

```{r eval=FALSE}

# data %>%
#   ungroup() %>%
#   distinct(TREATMENT)
# data %>%
#   filter(is.na(TREATMENT))
# data %>%
#   ungroup() %>%
#   distinct(CONTROL_TREATMENT)
# data %>%
#   ungroup() %>%
#   distinct(RESPONSE)
# data %>%
#   ungroup() %>%
#   filter(RESPONSE %in% c("NA_", "NA_NA"))
# data %>%
#   filter(!RESPONSE %in% c("NA_", "NA_NA")) %>%
#   select(ACCESSION_ID, RESPONSE, TREATMENT_MEAN) %>%
#   pivot_wider(names_from = RESPONSE, values_from = TREATMENT_MEAN) %>% View()
#   GGally::ggpairs(columns = c("shoot_weight", "root_weight", "total_weight", "shoot_height", "root_height", "total_height"))
# data %>%
#  filter(STUDYID == "Nervo et al.") %>%
#   select(STUDYID, MULTIPLE_ENDPOINT_CLUSTERID, TREATMENT_LABEL, TREATMENT, TREATMENT_MEAN, CONTROL_TREATMENT) %>%
#   arrange(MULTIPLE_ENDPOINT_CLUSTERID)
# data %>%
#   group_by(STUDYID) %>%
#   mutate(n_control = n_distinct(CONTROL_TREATMENT)) %>%
#   filter(n_control > 1)

```

**NOTE: Some studies measured three plant responses (e.g. shoot, root and total weight in Bornemissza 1970), but none of these are included in the weighted meta-analysis. The closest is Bang et al. 2005 that measured shoot and total weight, but I didn't calculate root weight.**

```{r Calculate effect size}
#| include: false

# Isolate the dung+beetles treatments with dung_only controls
data1 <- filter(data, 
                TREATMENT == "dung+beetles",   # Isolate the "dung+beetles" treatments from the TREATMENT column
                CONTROL_TREATMENT == "dung_only")   # AND isolate the "dung_only+beetles" treatments from the CONTROL_TREATMENT column   

# Filter out the studies that don't have the required measurements to calculate an effect size
data1 <- 
data1 %>% 
  drop_na(TREATMENT_N, 
          TREATMENT_MEAN, 
          TREATMENT_SD, 
          CONTROL_N, 
          CONTROL_MEAN,
          CONTROL_SD)   # Remove all studies that have 'NA' recorded for any of the six measurements required to calculate an effect size

data1 <- data1 %>%
  group_by(STUDYID) %>%
  mutate(SHAREDCONTROL_COUNT = n_distinct(SHAREDCONTROL_CLUSTERID),   # Count the number of unique control groups within each study
         SHARED_CONTROL_N = CONTROL_N / SHAREDCONTROL_COUNT)   # Divide the sample size of each control (CONTROL_N) by the number of shared controls within its study


## JAKE'S DEMO

# data1 <-
#   data %>%
#   
#   # Isolate the dung+beetles treatments with dung_only controls
#   filter(TREATMENT == "dung+beetles",   # Isolate the "dung+beetles" treatments from the TREATMENT column
#          CONTROL_TREATMENT == "dung_only") %>%  # AND isolate the "dung_only+beetles" treatments from the CONTROL_TREATMENT column   
#   # Filter out the studies that don't have the required measurements to calculate an effect size
#   drop_na(TREATMENT_N, 
#           TREATMENT_MEAN, 
#           TREATMENT_SD, 
#           CONTROL_N, 
#           CONTROL_MEAN,
#           CONTROL_SD) %>%   # Remove all studies that have 'NA' recorded for any of the six measurements required to calculate an effect size
#   group_by(STUDYID) %>%
#   mutate(SHAREDCONTROL_COUNT = n_distinct(SHAREDCONTROL_CLUSTERID),   # Count the number of unique control groups within each study
#          SHARED_CONTROL_N = CONTROL_N / SHAREDCONTROL_COUNT)   # Divide the sample size of each control (CONTROL_N) by the number of shared controls within its study



# Calculate an Effect Size (yi) and corresponding Sampling Variance (vi)
data1 <- escalc(measure="ROM",   # The effect size calculated will be log transformed ratio of means (metafor::escalc)
                m1i=TREATMENT_MEAN,   # m1i = the treatment mean (e.g. dung+beetles or dung_only)
                sd1i=TREATMENT_SD,   # sd1i = the treatment st.dev
                n1i=TREATMENT_N,   # n1i = the treatment sample size
                m2i=CONTROL_MEAN,   # m2i = the control mean (e.g. dung_only or soil_only)    
                sd2i=CONTROL_SD,   # sd2i = the control st.dev  
                n2i=SHARED_CONTROL_N,   # n2i = the control sample size (here we used our modified control sample size)
                data=data1,
                slab=paste(STUDYID, YEAR, sep=", "))
# Rename the Effect Size (yi) and corresponding Sampling Variance (vi)columns to make them more intuitive to read 
colnames(data1)[which(names(data1) == "yi")] <- "effect_size"   # The Effect Size column is now labelled 'effect_size'
colnames(data1)[which(names(data1) == "vi")] <- "effect_size_variance"   # The Sampling Variance is now labelled 'effect_size_variance'
# Calculate a standard error (sqrt(effect_size_variance)) and a precision value (1/standard error) (Bishop & Nakagawa, 2021)
data1$se <- sqrt(data1$effect_size_variance)
data1$precision <- 1/data1$se

```


As the dataset also contains non-independence in the form of **Shared measurements** and **Within-study temporal correlation** we create a value called MULTIPLE_ENDPOINT_CLUSTERID.
Whenever multiple plant traits were measured from the same plant at the same time, or when a single plant trait was measured from the same plant at different times, those outcomes received the same MULTIPLE_ENDPOINT_CLUSTERID. Sometimes studies had both Shared measurements and Within-study temporal correlation (e.g. Miranda et al., 1998).<br>

To deal with non-independence we use multi-level meta-analytic models with variance–covariance matrices.<br>
We generate variance-covariance matrices based on a clustered variable, which was MULTIPLE_ENDPOINT_CLUSTERID.<br>

As a sensitivity test, we produce three matrices, with effect sizes from the same experiment assumed to have a correlation of either 0.25, 0.5 or 0.75. A correlation value of 0 indicates no correlation and 1 a perfect correlation (Dougherty et al., 2022).<br>


```{r Correlation 0.25}
#| include: false

# Arrange data1 by MULTIPLE_ENDPOINT_CLUSTERID (ascending) so that the values from the matrices are associated with the correct effect sizes
data1 <- data1[order(data1$MULTIPLE_ENDPOINT_CLUSTERID), ]
# Generate the variance-covariance matrices using the metaAidR::make_VCV_matrix function
### Assumed correlation value = 0.25 correlation (low correlation)
variance.covariance.matrices_0.25 <- 
  make_VCV_matrix(data = data1,
                  V = "effect_size_variance",   # The variable containing effect size variances
                  cluster = "MULTIPLE_ENDPOINT_CLUSTERID",   # The variable indicating which effects belong to the same cluster. Outcomes with the same 'MULTIPLE_ENDPOINT_CLUSTERID' are assumed to be nonindependent (correlated).
                  obs = "ACCESSION_ID",   # The variable containing individual IDs for each value in V = "effect_size_variance"
                  type = "vcv",   # "vcv" indicates that a full variance-covariance matrix is needed for the non-independent blocks of variance values.
                  rho = 0.25)   # Setting the assumed correlation value at 0.25
is.positive.definite(variance.covariance.matrices_0.25)   # For a positive definite matrix, the eigenvalues should be positive, therefore we expect this function to return the output of 'TRUE'
model1_corr0.25 <- rma.mv(yi = effect_size,
                          V = variance.covariance.matrices_0.25,
                          random = list(~1 | STUDYID,   # Level 3: The highest grouping variable
                                        ~1 | ACCESSION_ID),   # Level 2: The second-highest grouping variable - nested within level 3
                   data = data1,
                   method = "REML")   # The restricted maximum-likelihood method is used to estimate the model parameters
summary(model1_corr0.25)

```


```{r Correlation 0.5}
#| include: false

### Assumed correlation value = 0.5 correlation (moderate correlation)
variance.covariance.matrices_0.5 <- 
  make_VCV_matrix(data = data1,
                  V = "effect_size_variance",
                  cluster = "MULTIPLE_ENDPOINT_CLUSTERID",
                  obs = "ACCESSION_ID",
                  type = "vcv",
                  rho = 0.5)   # Setting the assumed correlation value at 0.5
is.positive.definite(variance.covariance.matrices_0.5)
model1_corr0.5 <- rma.mv(yi = effect_size, 
                   V = variance.covariance.matrices_0.5, 
                   random = list(~1 | STUDYID,
                                 ~1 | ACCESSION_ID),
                   data = data1,
                   method = "REML")
summary(model1_corr0.5)

```


```{r Correlation 0.75}
#| include: false

### Assumed correlation value = 0.75 correlation (high correlation)
variance.covariance.matrices_0.75 <- make_VCV_matrix(data = data1,
                                                     V = "effect_size_variance",
                                                     cluster = "MULTIPLE_ENDPOINT_CLUSTERID",
                                                     obs = "ACCESSION_ID",
                                                     type = "vcv",
                                                     rho = 0.75)  # Setting the assumed correlation value at 0.75
is.positive.definite(variance.covariance.matrices_0.75)
model1_corr0.75 <- rma.mv(yi = effect_size,
                          V = variance.covariance.matrices_0.75,
                          random = list(~1 | STUDYID,
                                        ~1 | ACCESSION_ID), 
                   data = data1,
                   method = "REML")
summary(model1_corr0.75)

```

| Assumed correlation | *p*-value                                            | AIC                      |
|---------------------|------------------------------------------------------|--------------------------|
| 0.25 (low)          | `r round(tidy(model1_corr0.25)$p.value, digits = 3)` | `r AIC(model1_corr0.25)` |
| 0.5 (moderate)      | `r round(tidy(model1_corr0.5)$p.value, digits = 3)`  | `r AIC(model1_corr0.5)`  |
| 0.75 (high)         | `r round(tidy(model1_corr0.75)$p.value, digits = 3)` | `r AIC(model1_corr0.75)` |

The low (0.25) and moderate (0.5) correlations have a similar *p*-value and AIC. We decide to use the moderate (0.5) value as most studies do the same (Dougherty et al., 2022).<br>

**NOTE: I DIDN'T REALLY KNOW WHICH CORRELATION OPTION/VALUE TO GO WITH SO JUST PICKED 0.5 AS IT BASICALLY HAD THE JOINT-LOWEST AIC, IT'S WHAT OTHER PEOPLE DO AND IT'S THE DEFAULT CORRELATION VALUE IN THE PACKAGE. LET ME KNOW IF YOU THINK OTHERWISE**<br>

**Jakes note:** I have no strong opinion here. I'm not sure about comparing the models. My suggestion would be to follow Bishop and Nakagawa and present the 0.5 correlation results in the text and a .75 (or 0.9) correlation results in the supplementary material, assuming the results are similar! Thinking some more about this, how correlated do you expect the variables (different responses/different time points) to be for the same studies?

**NOTE: After re-analysing the higher the correlation, the better fit. **<br>


Now we test for different candidate random effects structures.<br>
**NOTE: I'm not 100% certain why some of these variables are tested now, and some others are tested later as moderators. At this point should I be testing a whole heap more variables than the 4 I've done below **<br>

```{r}
# Again, Arrange data1 by MULTIPLE_ENDPOINT_CLUSTERID (ascending)
data1 <- data1[order(data1$MULTIPLE_ENDPOINT_CLUSTERID), ]

RE <- rma.mv(yi = effect_size,
             V = variance.covariance.matrices_0.5,
             random = list(~1 | STUDYID, 
                           ~1 | ACCESSION_ID), 
             data = data1, 
             method = "ML")
summary(RE)
AIC(RE) #-43.92758

# test whether RESPONSE_AREA (shoot, root, total) should be a random effect
RE1 <- rma.mv(yi = effect_size,
              V = variance.covariance.matrices_0.5,
              random = list(~1 | STUDYID,
                            ~1 | RESPONSE_AREA,
                            ~1 | ACCESSION_ID),
              data = data1,
              method = "ML")
summary(RE1)
AIC(RE1) #-41.92758

# test whether RESPONSE_MEASUREMENT (height or weight) should be a random effect
RE2 <- rma.mv(yi = effect_size,
              V = variance.covariance.matrices_0.5,
              random = list(~1 | STUDYID,
                            ~1 | RESPONSE_MEASUREMENT,
                            ~1 | ACCESSION_ID),
              data = data1,
              method = "ML")
summary(RE2)
AIC(RE2) #-41.92758          The same as RE2???

# test whether RESPONSE (shoot, root or total height or shoot, root or total weight) should be a random effect
RE2 <- rma.mv(yi = effect_size,
              V = variance.covariance.matrices_0.5,
              random = list(~1 | STUDYID,
                            ~1 | RESPONSE,
                            ~1 | ACCESSION_ID),
              data = data1,
              method = "ML")
summary(RE2)
AIC(RE2) #-41.92758          The same as RE2???


# test whether DAYS_OF_PLANT_GROWTH should be a random effect
RE3 <- rma.mv(yi = effect_size,
              V = variance.covariance.matrices_0.5,
              random = list(~1 | STUDYID,
                            ~1 | DAYS_OF_PLANT_GROWTH,
                            ~1 | ACCESSION_ID),
              data = data1,
              method = "ML")
AIC(RE3) #-41.92758          The same as RE2???



# test whether PLANTS_GERMINATED_BEFORE_DUNG_ADDITION (Y or N) should be a random effect
RE4 <- rma.mv(yi = effect_size,
              V = variance.covariance.matrices_0.5,
              random = list(~1 | STUDYID,
                            ~1 | DAYS_OF_PLANT_GROWTH,
                            ~1 | ACCESSION_ID),
              data = data1,
              method = "ML")
AIC(RE4) #-41.92758          The same as RE2???


# Going by AIC values, RE is the best model


# compare our model (RE) to one assuming no dependence between effect sizes
# to do this we use effect_size_variance rather than the vcov matrix
null.RE <- rma.mv(yi = effect_size,
                  V = effect_size_variance,
                  random = list(~1 | STUDYID,
                                ~1 | ACCESSION_ID),
                  data = data1, 
                  method = "ML")
summary(null.RE)
AIC(null.RE) #-31.26941


# The original RE model (that accounts for non-dependence) is still the best fit

```

We found our best model, now report the results in the paper

```{r Multi-level meta-analysis - null model}
#| echo: false

# To report the results in our paper we convert method = "ML" to "REML"
RE.reml <- rma.mv(yi = effect_size,
             V = variance.covariance.matrices_0.5,
             random = list(~1 | STUDYID, 
                           ~1 | ACCESSION_ID), 
             data = data1, 
             method = "REML")

summary(RE.reml)

```

We then quantify the total variance (heterogeneity) attributable to each of the three levels.<br>
Here $I^{2}$ represents the amount of variation not attributable to sampling error. It is split into two parts; within clusters and between-cluster variation.<br>

```{r Testing the heterogeneity}

I2 <- as.data.frame(i2_ml(RE.reml))
I2

```

$I^{2}_{Level 2}$ (STUDYID) accounts for ~`r (I2_RE.reml["I2_STUDYID", 1] / I2_RE.reml["I2_Total", 1]) * 100`% of heterogeneity and $I^{2}_{Level 3}$ (ACCESSION_ID) accounts for ~`r (I2_RE.reml["I2_ACCESSION_ID", 1] / I2_RE.reml["I2_Total", 1]) * 100`% of heterogeneity.<br>
Therefore, this indicates that the majority of variance is attributable to heterogeneity variance within clusters on the second level.<br>

Now we calculate the overall estimate of plant growth increase from beetle activity. This is done by back-transforming the lnRR (Bishop & Nakagawa, 2021)<br>

```{r Overall average estimate of plant growth}
#| include: false

RE.reml.estimate <- exp(RE.reml$b[[1]]) - 1
RE.reml.estimate

```

So overall, in the studies we assessed, beetles increased plant growth by `r (round(RE.reml.estimate, 4)*100)`% compared to the 'dung_only' controls.<br>

Now we calculate the confidence intervals and prediction intervals.<br>
A prediction interval is the range within which the effect size of a new study would likely be, given this new study was selected at random from the same population of the studies already included in the meta-analysis (Spineli & Pandis, 2020).<br>

```{r Confidence and prediction intervals}
#| echo: false
RE.reml.res <- (mod_results(model = RE.reml,
                            mod = "1",
                            group = "STUDYID",
                            data = data1))
RE.reml.res

# We back-transform the prediction intervals for reporting in the paper.
RE.reml.lowerPR <- exp(RE.reml.res$mod_table$lowerPR) - 1
RE.reml.upperPR <- exp(RE.reml.res$mod_table$upperPR) - 1
# We back-transform the confidence intervals for reporting in the paper.
RE.reml.lowerCL <- exp(RE.reml.res$mod_table$lowerCL) - 1
RE.reml.upperCL <- exp(RE.reml.res$mod_table$upperCL) - 1
```


# Visualise the results

We can now make an orchard plot to visualise our results.<br>
The orchard plot contains confidence intervals (thick horizontal lines) and prediction intervals (thin horizontal lines) around a point estimate.<br>
The points represent scaled individual effect sizes.<br>
See Nakagawa et al. (2021) for more information on orchard plots.<br>

```{r Orchard plot}
#| echo: false
#| message: false
wes_colours <- wes_palettes$GrandBudapest1 # prepping the colours for later on
figure_orchard <- 
orchard_plot(RE.reml, 
             mod = "1",   # Set to "1" for intercept only model
             group = "STUDYID", 
             data = data1, 
             #k = FALSE,   # Mute the 'k = (number of effect sizes)' label
             xlab = "lnRR (effect size)") +
  scale_fill_manual(values=wes_colours[2]) +
  scale_colour_manual(values=wes_colours[3]) +  
  theme(panel.background = element_rect(fill = "transparent", 
                                        color = NA),   # background of the panel
        plot.background = element_rect(fill = "transparent", 
                                       color = NA),   # background of the plot
        legend.background = element_rect(fill = "transparent"),    # get rid of legend background
        legend.key = element_rect(fill = "transparent", 
                                  colour = NA))    # get rid of legend background
figure_orchard
# The orachaRd package/paper explainer has some good code for modifying orchard plots (https://github.com/daniel1noble/orchaRd) (Nakagawa et al., 2021)
ggsave(filename = (here("figures/orchard_plot.png")),
       plot = figure_orchard,
       bg = "transparent")   # Plot with a transparent background
# If there is a 'Graphics API version mismatch' when using ggsave try re-installing the ragg package
```

We can also make a caterpillar plot.<br>
The caterpillar plot shows effect sizes for each outcome along with their confidence intervals.
At the bottom it also shows a diamond that represents an overall mean along with confidence intervals (mean in the centre and CIs are the horizontal points of the diamond). This diamond is within the prediction intervals.<br>
See Nakagawa et al. (2021) for more information on caterpillar plots.<br>

```{r Caterpillar plot 1}
#| echo: false
figure_caterpillar <- 
caterpillars(RE.reml.res, 
             mod = "1", 
             group = "STUDYID", 
             data = data1,
             #k = FALSE,
             xlab = "effect size") +
  theme_classic() +
  scale_fill_manual(values=wes_colours) +
  scale_colour_manual(values=wes_colours) +  
  theme(panel.background = element_rect(fill = "transparent", 
                                        color = NA),
        plot.background = element_rect(fill = "transparent", 
                                       color = NA),
        legend.background = element_rect(fill = "transparent"),
        legend.key = element_rect(fill = "transparent", 
                                  colour = NA))
figure_caterpillar
ggsave(filename = (here("figures/caterpillar_plot.png")),
       plot = figure_caterpillar,
       bg = "transparent")
```

Here's another caterpillar plot that is more customisable.

```{r Caterpillar plot 2}
#| echo: false

# Calculate the confidence intervals for each individual study outcome
data1$lowerCI <- data1$effect_size - stats::qnorm(0.975) * sqrt(data1$effect_size_variance)
data1$upperCI <- data1$effect_size + stats::qnorm(0.975) * sqrt(data1$effect_size_variance)

# Data manipulation to enable geom_path() to plot confidence intervals (grouping them by ACCESSION_ID)
data1_long <-   # Subset of data1 with the columns 'data1, ACCESSION_ID, lowerCI, upperCI, effect_size'
  select(data1, ACCESSION_ID, lowerCI, upperCI, effect_size) 
data1_long <- data1_long[order(data1_long$effect_size), ]   # Sort dataset by effect size to make caterpillar plot look proper
data1_long$y_coord <- nrow(data1_long):1   # Create a list of numbers that will later act as y-coordinates in the plot
data1_long <-
  data1_long %>%
  pivot_longer(
    cols = ends_with("CI"),
    names_to = "CI"
  )
figure_caterpillar2 <- 
  ggplot() +
# Insert a vertical line at 0 indicating 'no effect'
  geom_vline(xintercept = 0,   # Set the position along the x-axis for the vertical line 
             colour = "grey",   # Set the line colour
             linetype = "longdash") +   # Make the line dashed
# Insert confidence intervals for each individual study outcome
geom_path(data = data1_long,
                           aes(x = value,   # Set the x-coordinate
                               y = y_coord,   # Set the y-coordinate
                               group = ACCESSION_ID),   # Group the coordinates (as each line on the plot is made from x- and y-coodinates on seperate lines in the dataset)
          colour = "dark grey",   # Set the line colour
          size = 1,   # Set the line size
          lineend = "round") +   # Make the ends of the line round (rather than the default square)
# Insert the effect size for each individual study outcome
geom_point(data = data1_long,
           aes(x = effect_size,   # Set the effect size
               y = y_coord),   # Set the y-coordinate
           col = "black",   # Set the point colour
           size = 1) +   # Set the point size
# Adding plot aesthetics
  theme_classic() +   # Make the plot background blank   
  theme(panel.background = element_rect(fill = "transparent"),   # Make the panel background transparent   
        plot.background = element_rect(fill = "transparent",   # Make the plot background transparent   
                                       color = NA),
        axis.text.y=element_blank(),   # Mute the y-axis text
        axis.ticks.y=element_blank()) +   # Mute the y-axis ticks
  labs(title = "Caterpillar plot",   # Label the plot
       x ="lnRR (effect size)",   # Label the x-axis (using 'expression' and 'paste' to incorporate an italic letter)
       y = NULL)   # Mute the y-axis label
# Set how far off the y-axis the diamond will be 
summary_bar_spacer <- -2
id <- "a"   # Create an arbitrary id for the upcoming diamond
x_coordinates <- c(RE.reml.lowerCL,   # Lower confidence interval for the overall study (left point of diamond)
                   RE.reml.estimate,   # Mean effect size for the overall study (horizontal mid-point of diamond) 
                   RE.reml.upperCL,   # Upper confidence interval for the overall study (right point of diamond)
                   RE.reml.estimate)   # Mean effect size for the overall study (horizontal mid-point of diamond)  
y_coordinates <- c(summary_bar_spacer,   # Vertical mid-point of the diamond (equal to summary_bar_spacer)
                   summary_bar_spacer+2,   # Top point of the diamond (equal to summary_bar_spacer + 2) 
                   summary_bar_spacer,   # Vertical mid-point of the diamond (equal to summary_bar_spacer)
                   summary_bar_spacer-2)   # Bottom point of the diamond (equal to summary_bar_spacer - 2) 
plot.diamond <- data.frame(id, x_coordinates, y_coordinates)   # Make a dataframe of the diamond coordinates
# Now add in the diamond and prediction intervals (it will be underneath the diamond)
figure_caterpillar2 <- figure_caterpillar2 +
  # Add the overall prediction intervals to the plot
  geom_linerange(data = plot.diamond,
                 aes(x = RE.reml.estimate,
                     y = summary_bar_spacer,
                     #y = 1,
                     xmax = RE.reml.upperPR,   # The lower prediction interval
                     xmin = RE.reml.lowerPR),   # The upper prediction interval
                 col = "grey",
                 size = 1) +
  
# Add the diamond to the caterpillar plot (it mill now be on top of the line for the overall prediction intervals)
    geom_polygon(data = plot.diamond,   # Using the dataframe with the diamond coordinates
               aes(fill = id,   # Fill diamond according to the (arbitrary) id (this is kind of meaningless when creating a single shape)
                   group = id,   # Group coordinates according to the (arbitrary) id (this is kind of meaningless when creating a single shape)
                   x = x_coordinates,   # Plot x-coordinates
                   y = y_coordinates),   # Plot y-coordinates
               fill = "black",   # Colour the centre of the diamond
               col = "black")   # Colour the edge of the diamond
figure_caterpillar2
ggsave(filename = (here("figures/caterpillar_plot2.png")),
       plot = figure_caterpillar2,
       bg = "transparent")
```

# Investigating moderators

Next we tested to see if moderators improved the model fit.<br> 
We couldn't test interactions between moderators due to missing combinations of levels, so we tested them individually.<br>
We compare each new model featuring a moderator with our original model (RE).<br>




**NOTE:**
**Below I've looked at a few moderators, but I'm mainly concerned with the ecological moderators looking at the beetle community:**<br>
**Beetle abundance**<br>
**Beetle biomass**<br>
**Beetle species richness**<br>
**Beetle functional group**<br>

**To a lesser extent I'm interested in the experimental moderators looking at:**<br>
**Plant clade**<br>
**Had the plants germinated before dung was added**<br>
**Dung quantity per beetle/surface area**<br>

**So in the final code/analysis I can probably drop some of those moderators**<br>

## Categorical moderators:

The results from testing moderators:<br>

We tested moderators individually by adding them to our null model (RE.reml), and comparing model performance with and without the moderator.<br>

**NOTE: These were all tested with an assumed correlation (rho) of 0.5. Also, I don't really know which of the models to go with or if any of the results are that meaningful. I've noted the points that look interesting to me, but I'm likely wrong**<br>


**Jake's comments:**   
* I think a key moderator that should be investigated is the response variable used  
* Generally, models should be run using maximum likelihood (method = "ML") for significance testing, and restricted maximum likelihood for estimating model parameters (method = "REML")  
* Not sure if the above applies when comparing models using AIC  
* Is there a reason for fitting (and comparing) intercept and no intercept models?  The AIC is the same for these when fit to categorical variables. I'd suggest not removing the intercept for the continuous moderator models (but there may be a reason to do so that I'm not aware of)  
* Try to avoid writing over objects by using different names (eg, you could use model2_FUNCTIONAL_GROUP instead of just model2)  
* When copying and pasting the same code three or more times, it may be worthwhile to write a function. See https://r4ds.had.co.nz/many-models.html for a way to run lots of models and extract the results. Have a crack if you think it will help and we can discuss more if you get stuck (only if you think it is useful!)  

I started to play with the RESPONSE moderator model. To be discussed:

```{r}
data1 %>%
  distinct(RESPONSE)
data %>%
  ungroup() %>%
  distinct(RESPONSE)
# root height gets lost in the filtering. Can this be calculated from other data?
# No - total_height and shoot_height found in separate studies
data1 %>%
  filter(RESPONSE %in%  c("total_height", "shoot_height"))%>%
  drop_na(TREATMENT_MEAN) %>%
  select(STUDYID, RESPONSE, TREATMENT_MEAN)
# total_weight will be the sum of root_weight and shoot_weight. I don't think all three variables should be included if derived from the same study. Check this
# All three are not included in the same study

# Only Bang et al that have total_weight - could then be recalculated as root_weight?
data1 %>%
  filter(RESPONSE %in%  c("root_weight", "shoot_weight", "total_weight"))%>%
  drop_na(TREATMENT_MEAN) %>%
  select(STUDYID, RESPONSE, TREATMENT_MEAN) %>%
  group_by(STUDYID) %>%
  distinct(RESPONSE)

# We should chat about the possible groupings for the response variables (above vs below ground, height vs weight). I feel this this is something we should consider carefully.
# Null model
model1_corr0.5_ML <- rma.mv(yi = effect_size,
                            V = variance.covariance.matrices_0.5,
                            random = list(~1 | STUDYID,
                                          ~1 | ACCESSION_ID),
                            data = data1,
                            method = "ML")

# Not sure why we are getting a warning here???
model_RESPONSE_ML <- rma.mv(yi = effect_size,
                            V = variance.covariance.matrices_0.5,
                            mods = ~RESPONSE - 1,
                            random = list(~1 | STUDYID,
                                          ~1 | ACCESSION_ID),
                            data = data1,
                            method = "ML")
anova(model_RESPONSE_ML, model1_corr0.5_ML)

model_RESPONSE_REML <- rma.mv(yi = effect_size,
                              V = variance.covariance.matrices_0.5,
                              mods = ~RESPONSE - 1,
                              random = list(~1 | STUDYID,
                                            ~1 | ACCESSION_ID),
                              data = data1,
                              method = "REML")
summary(model_RESPONSE_REML)

```


## RESPONSE_AREA (shoot, root or total)

```{r Beetle functional group - no moderator}
#| echo: false
#| warning: false
## Model 1 (no moderator):

# Our original/null model (maximum likelihood (RE), not restricted maximum likelihood (RE.reml))
summary(RE)

# Our model testing for RESPONSE_AREA as a moderator
RE_RESPONSE_AREA_ml <- rma.mv(yi = effect_size,
                              V = variance.covariance.matrices_0.5,
                              mods = ~RESPONSE_AREA - 1,
                              random = list(~1 | STUDYID,
                                            ~1 | ACCESSION_ID),
                              data = data1,
                              method = "ML")

anova(RE_RESPONSE_AREA_ml, RE)   # Full is with moderator, Reduced is without (null)

# Re-fit the model with REML for reporting estimates in the paper
RE_RESPONSE_AREA_reml <- rma.mv(yi = effect_size,
                                V = variance.covariance.matrices_0.5,
                                mods = ~RESPONSE_AREA - 1,
                                random = list(~1 | STUDYID,
                                              ~1 | ACCESSION_ID),
                                data = data1,
                                method = "REML")
summary(RE_RESPONSE_AREA_reml)

```

**NOTE: RESPONSE_AREA is not influential**


## RESPONSE_MEASUREMENT (height or weight)

```{r Beetle functional group - no moderator}
#| echo: false
#| warning: false
## Model 1 (no moderator):

# Our original/null model (maximum likelihood (RE), not restricted maximum likelihood (RE.reml))
summary(RE)

# Our model testing for RESPONSE_MEASUREMENT as a moderator
RE_RESPONSE_MEASUREMENT_ml <- rma.mv(yi = effect_size,
                              V = variance.covariance.matrices_0.5,
                              mods = ~RESPONSE_MEASUREMENT - 1,
                              random = list(~1 | STUDYID,
                                            ~1 | ACCESSION_ID),
                              data = data1,
                              method = "ML")

anova(RE_RESPONSE_MEASUREMENT_ml, RE)   # Full is with moderator, Reduced is without (null)

# Re-fit the model with REML for reporting estimates in the paper
RE_RESPONSE_MEASUREMENT_reml <- rma.mv(yi = effect_size,
                                V = variance.covariance.matrices_0.5,
                                mods = ~RESPONSE_MEASUREMENT - 1,
                                random = list(~1 | STUDYID,
                                              ~1 | ACCESSION_ID),
                                data = data1,
                                method = "REML")
summary(RE_RESPONSE_MEASUREMENT_reml)

```

**NOTE: RESPONSE_MEASUREMENT is not influential**


## RESPONSE (shoot, root or total height or shoot, root or total weight)

```{r Beetle functional group - no moderator}
#| echo: false
#| warning: false
## Model 1 (no moderator):

# Our original/null model (maximum likelihood (RE), not restricted maximum likelihood (RE.reml))
summary(RE)

# Our model testing for RESPONSE as a moderator
RE_RESPONSE_ml <- rma.mv(yi = effect_size,
                         V = variance.covariance.matrices_0.5,
                         mods = ~RESPONSE - 1,
                         random = list(~1 | STUDYID,
                                       ~1 | ACCESSION_ID),
                         data = data1,
                         method = "ML")

anova(RE_RESPONSE_ml, RE)   # Full is with moderator, Reduced is without (null)

# Re-fit the model with REML for reporting estimates in the paper
RE_RESPONSE_reml <- rma.mv(yi = effect_size,
                           V = variance.covariance.matrices_0.5,
                           mods = ~RESPONSE - 1,
                           random = list(~1 | STUDYID,
                                         ~1 | ACCESSION_ID),
                           data = data1,
                           method = "REML")
summary(RE_RESPONSE_reml)

```

**NOTE: RESPONSE is not influential**


## Beetle functional group

**NOTE: NAs omitted**

```{r Beetle functional group - no moderator}
#| echo: false
#| warning: false

# Our original/null model (maximum likelihood (RE), not restricted maximum likelihood (RE.reml))
summary(RE)

# Our model testing for FUNCTIONAL_GROUP as a moderator
RE_FUNCTIONAL_GROUP_ml <- rma.mv(yi = effect_size,
                                 V = variance.covariance.matrices_0.5,
                                 mods = ~FUNCTIONAL_GROUP - 1,
                                 random = list(~1 | STUDYID,
                                               ~1 | ACCESSION_ID),
                                 data = data1,
                                 method = "ML")
anova(RE_FUNCTIONAL_GROUP_ml, RE)   # Full is with moderator, Reduced is without (null)

# Re-fit the model with REML for reporting estimates in the paper
RE_FUNCTIONAL_GROUP_reml <- rma.mv(yi = effect_size,
                           V = variance.covariance.matrices_0.5,
                           mods = ~FUNCTIONAL_GROUP - 1,
                           random = list(~1 | STUDYID,
                                         ~1 | ACCESSION_ID),
                           data = data1,
                           method = "REML")
summary(RE_FUNCTIONAL_GROUP_reml)

```


## Plant clade

```{r Beetle functional group - no moderator}
#| echo: false
#| warning: false

# Our original/null model (maximum likelihood (RE), not restricted maximum likelihood (RE.reml))
summary(RE)

# Our model testing for CLADE as a moderator
RE_CLADE_ml <- rma.mv(yi = effect_size,
                                 V = variance.covariance.matrices_0.5,
                                 mods = ~CLADE - 1,
                                 random = list(~1 | STUDYID,
                                               ~1 | ACCESSION_ID),
                                 data = data1,
                                 method = "ML")
anova(RE_CLADE_ml, RE)   # Full is with moderator, Reduced is without (null)

# Re-fit the model with REML for reporting estimates in the paper
RE_CLADE_reml <- rma.mv(yi = effect_size,
                           V = variance.covariance.matrices_0.5,
                           mods = ~CLADE - 1,
                           random = list(~1 | STUDYID,
                                         ~1 | ACCESSION_ID),
                           data = data1,
                           method = "REML")
summary(RE_CLADE_reml)

```

**NOTE: CLADE is not influential**


## Plants germinated before dung addition

```{r Beetle functional group - no moderator}
#| echo: false
#| warning: false

# Our original/null model (maximum likelihood (RE), not restricted maximum likelihood (RE.reml))
summary(RE)

# Our model testing for PLANTS_GERMINATED_BEFORE_DUNG_ADDITION as a moderator
RE_PLANTS_GERMINATED_BEFORE_DUNG_ADDITION_ml <- rma.mv(yi = effect_size,
                                                       V = variance.covariance.matrices_0.5,
                                                       mods = ~PLANTS_GERMINATED_BEFORE_DUNG_ADDITION - 1,
                                                       random = list(~1 | STUDYID,
                                                                     ~1 | ACCESSION_ID),
                                                       data = data1,
                                                       method = "ML")
anova(RE_PLANTS_GERMINATED_BEFORE_DUNG_ADDITION_ml, RE)   # Full is with moderator, Reduced is without (null)

# Re-fit the model with REML for reporting estimates in the paper
RE_PLANTS_GERMINATED_BEFORE_DUNG_ADDITION_reml <- rma.mv(yi = effect_size,
                                                         V = variance.covariance.matrices_0.5,
                                                         mods = ~PLANTS_GERMINATED_BEFORE_DUNG_ADDITION - 1,
                                                         random = list(~1 | STUDYID,
                                                                       ~1 | ACCESSION_ID),
                                                         data = data1,
                                                         method = "REML")
summary(RE_PLANTS_GERMINATED_BEFORE_DUNG_ADDITION_reml)

```

**NOTE: PLANTS_GERMINATED_BEFORE_DUNG_ADDITION is not influential**






















Secondly, with a moderator

```{r Beetle functional group - with a moderator}
#| echo: false
#| warning: false
## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~FUNCTIONAL_GROUP, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)
```

Finally, with a moderator (and the intercept removed)

```{r Beetle functional group - with a moderator and intercept removed}
#| echo: false
#| warning: false
## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~FUNCTIONAL_GROUP - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)
model3.res <- (mod_results(model3,
                           group = "STUDYID",
                           data = data1,
                           mod = "FUNCTIONAL_GROUP"))
```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE: I've plotted model 3 below, but Model 1 has a lower AIC value which would mean it's a better fit? It seems like rollers may be improving plant growth?**

Orchard plot - with a moderator (and the intercept removed)

```{r Plotting Beetle functional group}
#| echo: false
FUNCTIONAL_GROUP.plot <- 
  orchard_plot(model3.res,
               mod = "1",
               group = "STUDYID",
               data = data1,
               xlab = "lnRR (effect size)") +
    labs(title = "Beetle functional group influencing plant growth",
         x = "Beetle functional group")   # The x-axis and y-axis labels are counter-intuitive in this plot
FUNCTIONAL_GROUP.plot
```

## Plant clade

Firstly, with no moderator

```{r Plant clade - no moderator}
#| echo: false
#| warning: false
## Model 1 (no moderator):
summary(model1)
#tidy(model1)
```

Secondly, with a moderator

```{r Plant clade - with a moderator}
#| echo: false
#| warning: false
## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~CLADE, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)
```

Finally, with a moderator (and the intercept removed)

```{r Plant clade - with a moderator and intercept removed}
#| echo: false
#| warning: false
## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~CLADE - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)
model3.res <- (mod_results(model3,
                           group = "STUDYID",
                           data = data1,
                           mod = "CLADE"))
```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE: I've plotted model 3 below, but Model 1 has a lower AIC value which would mean it's a better fit? Also, monocots have the biggest increase in plant growth? Seems like dicots drag the effect size down**

Orchard plot - with a moderator (and the intercept removed)

```{r Plotting Plant clade}
#| echo: false
CLADE.plot <- 
  orchard_plot(model3.res,
               mod = "1",
               group = "STUDYID",
               data = data1,
               xlab ="lnRR (effect size)") +
    labs(title = "Plant clade influencing plant growth",
         x = "Plant clade")
CLADE.plot
```


## Had the plants germinated before dung was added

Firstly, with no moderator

```{r Had the plants germinated before dung was added - no moderator}
#| echo: false
#| warning: false
## Model 1 (no moderator):
summary(model1)
#tidy(model1)
```

Secondly, with a moderator

```{r Had the plants germinated before dung was added - with a moderator}
#| echo: false
#| warning: false
## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~PLANTS_GERMINATED_BEFORE_DUNG_ADDITION, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)
```

Finally, with a moderator (and the intercept removed)

```{r Had the plants germinated before dung was added - with a moderator and intercept removed}
#| echo: false
#| warning: false
## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~PLANTS_GERMINATED_BEFORE_DUNG_ADDITION - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)
model3.res <- (mod_results(model3,
                           group = "STUDYID",
                           data = data1,
                           mod = "PLANTS_GERMINATED_BEFORE_DUNG_ADDITION"))
```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE: Model 1 is the best fit. Plants being established prior to dung addition is not influential to plant growth?**

Orchard plot - with a moderator (and the intercept removed)

```{r Plotting Had the plants germinated before dung was added}
#| echo: false
PLANTS_GERMINATED_BEFORE_DUNG_ADDITION.plot <- 
  orchard_plot(model3.res,
               mod = "1",
               group = "STUDYID",
               data = data1,
               xlab ="lnRR (effect size)") +
    labs(title = "Plant germination time influencing plant growth",
         x = "Plant germinated before dung addition")
PLANTS_GERMINATED_BEFORE_DUNG_ADDITION.plot
```


# Numerical moderators:

**Jake's comment:** Any idea what's happening with the outliers?

## Number of beetles

Firstly, with no moderator

```{r Number of beetles - no moderator}
#| echo: false
#| warning: false
## Model 1 (no moderator):
summary(model1)
#tidy(model1)
```

Secondly, with a moderator

```{r Number of beetles - with a moderator}
#| echo: false
#| warning: false
## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~NUMBER_OF_BEETLES, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)
```

Finally, with a moderator (and the intercept removed)

```{r Number of beetles - with a moderator and intercept removed}
#| echo: false
#| warning: false
## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~NUMBER_OF_BEETLES - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)
```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE 1: I think model 1 is the best fit, but maybe there are some outliers here to be removed and then re-analyse?**<br>

Visualise data

**NOTE 2: I should make a trendline for the plot using the predict function, however I struggle a little with implementing predict and it will take me time. Therefore, I've left the trendlines out for now, but if these numerical moderator plots end up in the final analysis I'll spend the time to get them working**<br>

```{r Plotting  Number of beetles}
#| echo: false
#| warning: false
NUMBER_OF_BEETLES.plot <- 
  ggplot(data = data1, 
         aes(x = NUMBER_OF_BEETLES,
             y = effect_size)) +
  geom_point(aes(size = precision),   # Point size corresponds to effect size variance
             shape = 21,   # Shape 21 allows for a different coloured ring around each point
             fill = NA,   # Remove the fill colour of the points
             color = "black") +   # Make the outer ring of the points black
  labs(title = "Number of beetles influencing plant growth",   # Lab title 
       x = "Number of beetles",   # X-axis label
       y ="lnRR (effect size)",   # Y-axis label - using an expression to insert an italic letter
       size = "Precision\n1/SE") +
  theme_classic() +
  theme(panel.background = element_rect(fill = "transparent"),   # bg of the panel
        plot.background = element_rect(fill = "transparent", 
                                       color = NA))
NUMBER_OF_BEETLES.plot
```


## Total biomass of beetles

Firstly, with no moderator

```{r Total biomass of beetles - no moderator}
#| echo: false
#| warning: false
## Model 1 (no moderator):
summary(model1)
#tidy(model1)
```

Secondly, with a moderator

```{r Total biomass of beetles - with a moderator}
#| echo: false
#| warning: false
## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~TOTAL_BEETLE_WEIGHT_IN_MG, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)
```

Finally, with a moderator (and the intercept removed)

```{r Total biomass of beetles - with a moderator and intercept removed}
#| echo: false
#| warning: false
## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~TOTAL_BEETLE_WEIGHT_IN_MG - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)
```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE 1: I think model 1 is the best fit, but maybe there are some outliers here to be removed and then re-analyse?**<br>

Visualise data

```{r Plotting Total biomass of beetles}
#| echo: false
#| warning: false
TOTAL_BEETLE_WEIGHT_IN_MG.plot <- 
  ggplot(data = data1, 
         aes(x = TOTAL_BEETLE_WEIGHT_IN_MG,
             y = effect_size)) +
  geom_point(aes(size = precision),
             shape = 21,
             fill = NA,
             color = "black") +
  labs(title = "Total beetle biomass influencing plant growth",
       x = "Total beetle biomass (mg)",
       y ="lnRR (effect size)",
       size = "Precision\n1/SE") +
  theme_classic() +
  theme(panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent",
                                       color = NA))
TOTAL_BEETLE_WEIGHT_IN_MG.plot
```


## Beetle species richness

Firstly, with no moderator

```{r Beetle species richness - no moderator}
#| echo: false
#| warning: false
## Model 1 (no moderator):
summary(model1)
#tidy(model1)
```

Secondly, with a moderator

```{r Beetle species richness - with a moderator}
#| echo: false
#| warning: false
## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~SPECIES_RICHNESS, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)
```

Finally, with a moderator (and the intercept removed)

```{r Beetle species richness - with a moderator and intercept removed}
#| echo: false
#| warning: false
## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~SPECIES_RICHNESS - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)
model3.res <- (mod_results(model3,
                           group = "STUDYID",
                           data = data1,
                           mod = "SPECIES_RICHNESS"))
```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE: I've plotted model 3 below, but Model 1 has a lower AIC value which would mean it's a better fit? It seems like plant growth may be improving when 4 beetle species are present?**

Orchard plot - with a moderator (and the intercept removed)

```{r Plotting Beetle species richness 1}
#| echo: false
SPECIES_RICHNESS.plot <- 
  ggplot(data = data1, 
         aes(x = SPECIES_RICHNESS,
             y = effect_size)) +
  geom_point(aes(size = precision),
             shape = 21,
             fill = NA,
             color = "black") +
  labs(title = "Species richness influencing plant growth",
       x = "Species richness",
       y ="lnRR (effect size)",
       size = "Precision\n1/SE") +
  theme_classic() +
  theme(panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent",
                                       color = NA))
SPECIES_RICHNESS.plot
```

Visualise data

```{r Plotting Beetle species richness 2}
#| echo: false
SPECIES_RICHNESS.plot2 <- 
  orchard_plot(model3.res,
               mod = "1",
               group = "STUDYID",
               data = data1,
               xlab ="lnRR (effect size)") +
    labs(title = "Species richness influencing plant growth",
         x = "Species richness")                                                # This x label actually corresponds to the y axis
SPECIES_RICHNESS.plot2
```


## Amount of dung per surface area

Firstly, with no moderator

```{r Amount of dung per surface area - no moderator}
#| echo: false
#| warning: false
## Model 1 (no moderator):
summary(model1)
#tidy(model1)
```

Secondly, with a moderator

```{r Amount of dung per surface area - with a moderator}
#| echo: false
#| warning: false
## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~GRAMS_DUNG_PER_CM_SQUARED, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)
```

Finally, with a moderator (and the intercept removed)

```{r Amount of dung per surface area - with a moderator and intercept removed}
#| echo: false
#| warning: false
## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~GRAMS_DUNG_PER_CM_SQUARED - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)
```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE: ^ Model 2 looks best here? Although maybe it would be best to remove that study with the high grams per dung**


```{r Plotting Amount of dung per surface area}
#| echo: false
#| warning: false
GRAMS_DUNG_PER_CM_SQUARED.plot <- 
  ggplot(data = data1, 
         aes(x = GRAMS_DUNG_PER_CM_SQUARED,
             y = effect_size)) +
  geom_point(aes(size = precision),
             shape = 21,
             fill = NA,
             color = "black") +
  labs(title = "Dung quantity influencing plant growth",
       x = "Dung quantity (grams per cm squared)",
       y ="lnRR (effect size)",
       size = "Precision\n1/SE") +
  theme_classic() +
  theme(panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent",
                                       color = NA))

GRAMS_DUNG_PER_CM_SQUARED.plot
```


## Amount of dung per beetle

Firstly, with no moderator

```{r Amount of dung per beetle - no moderator}
#| echo: false
#| warning: false
## Model 1 (no moderator):
summary(model1)
#tidy(model1)
```

Secondly, with a moderator

```{r Amount of dung per beetle - with a moderator}
#| echo: false
#| warning: false
## Model 2 (with a moderator):
model2 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~GRAMS_DUNG_PER_BEETLE, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model2)
#tidy(model2)
```

Finally, with a moderator (and the intercept removed)

```{r Amount of dung per beetle - with a moderator and intercept removed}
#| echo: false
#| warning: false
## Model 3 (with a moderator and intercept removed):
model3 <- rma.mv(yi = effect_size, 
                 V = variance.covariance.matrices_0.5,
                 mods = ~GRAMS_DUNG_PER_BEETLE - 1, 
                 random = list(~1 | STUDYID, 
                               ~1 | ACCESSION_ID), 
                 data = data1,
                 method = "REML")
summary(model3)
#tidy(model3)
```

Visualise data

```{r Plotting Amount of dung per beetle}
#| echo: false
#| warning: false
GRAMS_DUNG_PER_BEETLE.plot <- 
  ggplot(data = data1, 
         aes(x = GRAMS_DUNG_PER_BEETLE,
             y = effect_size)) +
  geom_point(aes(size = precision),
             shape = 21,
             fill = NA,
             color = "black") +
  labs(title = "Dung quantity influencing plant growth",
       x = "Dung quantity (grams per beetle)",
       y ="lnRR (effect size)",
       size = "Precision\n1/SE") +
  theme_classic() +
  theme(panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent",
                                       color = NA))
GRAMS_DUNG_PER_BEETLE.plot
```

| Model                                                  | AIC             |
|--------------------------------------------------------|-----------------|
| Model 1 - with no moderator                            | `r AIC(model1)` |
| Model 2 - with a moderator                             | `r AIC(model2)` |
| Model 3 - with a moderator (and the intercept removed) | `r AIC(model3)` |

**NOTE: Model 1 seems to be the best fit**

**NOTE: Overall I don't think the moderators investigated are that influential**


## Publication bias

Next we check for publication bias and it's recommended to do this with more than one source (Nakagawa et al., 2017).<br>

Firstly we produce a funnel plot, and as the moderators were not influential these are omitted.<br>

Nakagawa & Lagisz (2016) give brief instructions on interpreting a funnel plot.


```{r Funnel plot without moderators}
#| echo: false
#| warning: false
#png(file = (here("figures/funnel_plot.png")))
#figure_funnel <-
funnel(model1,
       vi = variance.covariance.matrices_0.5,
       yaxis = "seinv",   # The measure of precision for the y-axis
       main = "Model 1 - Doesn't include moderators",   # Main title
       legend = TRUE,
       back	= "transparent",   # colour to use for the background of the plotting region
       hlines = "lightgray",   # colour of the horizontal reference lines
       col = "black",   # specify point colour
       bg = "transparent")   # specify the background colour for the plot
#figure_funnel
#dev.off()
# This code chunk only seems to work on the first execution for some reason
```

**NOTE: I might have to trim this down as it looks a bit weird, there seems to be no skew though**<br>
**NOTE: This figure has been troublesome to knit**<br>

**Jake's comment:** There seemed to be an issue with saving the figure as an object.

As a second assessment of publication bias, we use Egger's regression (see Egger et al. (1997)), with the standard
error, SE (sqrt(effect_size_variance)) as a moderator. A significant slope for SE indicates statistically significant funnel asymmetry after controlling for all other variables in the model (Bishop & Nakagawa, 2021).<br>

```{r Eggers test}
#| echo: false
#| warning: false
# sqrt(vi) = sampling standard error
# test is if sqrt(vi) moderator is statistically sig = asymmetry in funnel plot
egger <- rma.mv(yi = effect_size,
                V = variance.covariance.matrices_0.5,
                mods = sqrt(effect_size_variance),
                random = list(~1 | STUDYID,
                              ~1 | ACCESSION_ID),
                data = data1,
                method = "REML")
summary(egger)
```

The results indicate funnel symmetry. Now to visualise the Egger's regression.

```{r visualise Eggers test}
#| echo: false
#| warning: false
# A bubble plot showing a predicted loess line (dashed black line) for the contentious variable ‘sqrt(effect_size_variance)‘, with their 95% confidence
# regions (orange dotted lines) and 95% prediction regions (blue dotted lines) with observed effect sizes based on
# various sample sizes. Note that the lines are not linear as these are based on multivariate predictions of the data points.
egger_predict <- predict.rma(egger)
figure_eggers <- data1 %>% 
  mutate(ymin = egger_predict$ci.lb, 
         ymax = egger_predict$ci.ub,
         ymin2 = egger_predict$cr.lb, 
         ymax2 = egger_predict$cr.ub, 
         pred = egger_predict$pred) %>%
ggplot(aes(x = sqrt(effect_size_variance), 
           y = effect_size, size = sqrt(1/effect_size_variance))) + 
  geom_point(shape = 21,
             fill = (values=wes_colours[2])) + 
  geom_smooth(aes(y = ymin2), 
              method = "loess",
              se = FALSE, 
              lty = "dotted", 
              lwd = 0.75, 
              colour = "#0072B2") +
  geom_smooth(aes(y = ymax2), 
              method = "loess", 
              se = FALSE,
              lty = "dotted", 
              lwd = 0.75, 
              colour = "#0072B2") + 
  geom_smooth(aes(y = ymin),
              method = "loess", 
              se = FALSE, 
              lty = "dotted", 
              lwd = 0.75,
              colour = "#D55E00") + 
  geom_smooth(aes(y = ymax), 
              method = "loess",
              se = FALSE, 
              lty = "dotted", 
              lwd = 0.75, 
              colour = "#D55E00") +
  geom_smooth(aes(y = pred), 
              method = "loess", 
              se = FALSE,
              lty = "dashed", 
              lwd = 1, 
              colour = "black") + 
  labs(x = "sqrt(sampling variance)",
       y = "lnRR (effect size)", 
       size = "Precision (1/SE)") + 
  guides(fill = "none",
         colour = "none") + # themes
  theme_bw()+  
  theme(panel.background = element_rect(fill = "transparent", 
                                        color = NA),
        plot.background = element_rect(fill = "transparent", 
                                       color = NA),
        legend.background = element_rect(fill = "transparent"),
        legend.key = element_rect(fill = "transparent", 
                                  colour = NA))
figure_eggers
ggsave(filename = (here("figures/eggers_plot.png")),
       plot = figure_eggers,
       bg = "transparent")
```


**NOTE: It might be possible to conduct an un-weighted meta-analysis. It's probably unnecessary given the funnel plot and eggers test, but could be a good way to incorporate those studies that were excluded from the analysis for not having standard deviations/errors**


## Un-weighted meta-analysis

14 reports/studies were included in this meta-analysis, but there were an additional 10 reports/studies that lacked the full information needed to calculate a SMD (mean, SD & sample size).<br>

**Jake's comment**: this seems easier than imputing the missing standard deviations. Note that there is a warning message here (and some of the models above) that studies with NAs were ommitted from model fitting. That could just be a generic warning, but it will be worth checking that there's nothing causing any of the data to be excluded.

``` {r}
#| include: false
# Trying an un-weighted meta-analysis
# Nakagawa et al., 2017: 
# 'a lack of the information required to obtain sampling variance for a portion of the dataset (for example, missing standard deviations)' ...[to deal with this]... 'the authors should run models both with and without data with sampling variance information; note that without sampling variance (that is, unweighted meta-analysis) the analysis becomes a normal linear model (Morrissey, 2016)'
# Osburn & Callender (1992) have a formula for an unweighted meta-analysis.
# I think an un-weighted meta-analysis is also called an 'informal' or a 'bare-bones' meta-analysis
# Information also here: https://www.metafor-project.org/doku.php/analyses:miller1978?s[]=unweighted
model1
res <- rma(yi = effect_size, 
           vi = effect_size_variance, 
           method="EE", 
           data=data2, 
           weighted=FALSE)   # Specify an un-weighted meta-analysis
res
funnel(res)
```




Could try looking at soil_only controls too

``` {r}

# Isolate the dung+beetles treatments with soil_only controls
data2 <- filter(data, 
                TREATMENT == "dung+beetles",   # Isolate the "dung+beetles" treatments from the TREATMENT column
                CONTROL_TREATMENT == "soil_only")   # AND isolate the "dung_only+beetles" treatments from the CONTROL_TREATMENT column   

# Isolate the dung_only treatments with soil_only controls
data3 <- filter(data, 
                TREATMENT == "dung_only",   # Isolate the "dung+beetles" treatments from the TREATMENT column
                CONTROL_TREATMENT == "soil_only")   # AND isolate the "dung_only+beetles" treatments from the CONTROL_TREATMENT column   

```



## References:
Bishop, J., & Nakagawa, S. (2021). Quantifying crop pollinator dependence and its heterogeneity using multi-level meta-analysis. Journal of Applied Ecology, 58(5), 1030–1042. https://doi.org/10.1111/1365-2664.13830<br>
Dougherty, L. R., Skirrow, M. J. A., Jennions, M. D., & Simmons, L. W. (2022). Male alternative reproductive tactics and sperm competition: a meta-analysis. Biological Reviews, 97, 1365–1388. https://doi.org/10.1111/brv.12846<br>
Hedges, L.V., Gurevitch, J. and Curtis, P.S. (1999), THE META-ANALYSIS OF RESPONSE RATIOS IN EXPERIMENTAL ECOLOGY. Ecology, 80: 1150-1156. https://doi.org/10.1890/0012-9658(1999)080[1150:TMAORR]2.0.CO;2
Johnson, S. N., Lopaticki, G., Barnett, K., Facey, S. L., Powell, J. R., & Hartley, S. E. (2016). An insect ecosystem engineer alleviates drought stress in plants without increasing plant susceptibility to an above-ground herbivore. Functional Ecology, 30(6), 894–902. https://doi.org/10.1111/1365-2435.12582<br>
Manning, P., Slade, E. M., Beynon, S. A., & Lewis, O. T. (2017). Effect of dung beetle species richness and chemical perturbation on multiple ecosystem functions. Ecological Entomology, 42(5), 577–586. https://doi.org/10.1111/een.12421<br>
Miranda, C. H. B., Santos, J. C. C., & Bianchin, I. (1998). Contribuição de Onthophagus gazella à melhoria da fertilidade do solo pelo enterrio de massa fecal bovina fresca. Brazilian Journal of Animal Science, 27, 681–685.<br>
Nakagawa, S., & Lagisz, M. (2016). Visualizing unbiased and biased unweighted meta-analyses. Journal of Evolutionary Biology, 29(10), 1914–1916. https://doi.org/10.1111/jeb.12945<br>
Nakagawa, Shinichi, & Santos, E. S. A. (2012). Methodological issues and advances in biological meta-analysis. Evolutionary Ecology, 26(5), 1253–1274. https://doi.org/10.1007/s10682-012-9555-5<br>
Nervo, B., Caprio, E., Celi, L., Lonati, M., Lombardi, G., Falsone, G., Iussig, G., Palestrini, C., Said-Pullicino, D., & Rolando, A. (2017). Ecological functions provided by dung beetles are interlinked across space and time: Evidence from 15N isotope tracing. Ecology, 98(2), 433–446. https://doi.org/10.1002/ecy.1653<br>
Noble, D. W. A., Lagisz, M., O’dea, R. E., & Nakagawa, S. (2017). Nonindependence and sensitivity analyses in ecological and evolutionary meta‐analyses. Molecular Ecology, 26, 2410–2425. https://doi.org/doi.org/10.1111/mec.14031<br>
Page, M. J., McKenzie, J. E., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., Shamseer, L., Tetzlaff, J. M., Akl, E. A., Brennan, S. E., Chou, R., Glanville, J., Grimshaw, J. M., Hróbjartsson, A., Lalu, M. M., Li, T., Loder, E. W., Mayo-Wilson, E., McDonald, S., … Moher, D. (2021). The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. The BMJ, 372. https://doi.org/10.1136/bmj.n71<br>
Spineli, L. M., & Pandis, N. (2020). Prediction interval in random-effects meta-analysis. American Journal of Orthodontics and Dentofacial Orthopedics, 157(4), 586–588. https://doi.org/10.1016/j.ajodo.2019.12.011<br>
Viechtbauer, W. (2010). Conducting meta-analyses in {R} with the {metafor} package. Journal of Statistical Software, 36(3), 1–48. https://doi.org/10.18637/jss.v036.i03<br>